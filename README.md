# LLM Recommender System

## Overview

This repository contains the code, data, and methodology used to build an advanced recommendation system leveraging Large Language Models (LLMs) and traditional collaborative filtering techniques. The project predicts how users would rate books they haven’t reviewed yet, enhancing user experience by offering personalized recommendations.

The system adopts a hybrid approach:
- **Collaborative Filtering**: Recommends books based on user-item interaction patterns.
- **Content-Based Filtering with LLMs**: Uses LLMs to process user reviews, generating semantic embeddings to enhance recommendation accuracy.

This hybrid approach combines the personalization strength of collaborative filtering with the language understanding capabilities of LLMs.

## Key Achievements

- Collaborative filtering model RMSE: **0.94**
- Content-based filtering model RMSE: **0.69**
- LLM-powered review vectorization enables deeper understanding of user preferences.

---

## Dataset

The dataset is sourced from Kaggle and contains detailed **Amazon book reviews**. This dataset is tailored to the book category, making it ideal for building a recommendation system in the domain of literature.

### Dataset Details

- **Total Rows**: 3,105,370 (subsampled for this project)
- **Number of Features**: 15
- **Domain**: Books
- **Columns**:
  - `customer_id`: Unique identifier for each customer.
  - `product_id`: Unique identifier for each book.
  - `star_rating`: The rating given by the user (1-5 stars).
  - `review_headline` and `review_body`: Textual content from the reviews.

The rich textual data makes it a good fit for natural language processing tasks, which are key for extracting meaningful insights and generating vectors through LLMs.

You can download the dataset from [Kaggle](https://www.kaggle.com/datasets/beaglelee/amazon-reviews-us-books-v1-02-tsv-zip).

---

## LLM Integration

In this project, Large Language Models (LLMs) play a critical role in the **content-based filtering** part of the recommendation system. We leverage models like **BERT** and **GPT** to process user reviews, extracting meaningful semantic embeddings that reflect the content and sentiment of the reviews.

### Steps in LLM Integration:

1. **Review Preprocessing**: Cleaning, tokenizing, and preparing reviews for the LLM.
2. **Embedding Generation**: Passing each review through an LLM to generate a vector representation.
3. **Similarity Computation**: Calculating the similarity between books based on their LLM-generated embeddings to improve recommendation accuracy.

By using LLMs, we gain a deeper understanding of user preferences, resulting in more accurate and personalized recommendations.

---

## Project Structure (Main Files and Folders)

```
├── data/                             # Contains data files for the project
│   ├── amazon_reviews_subset.csv      # Subset of Amazon reviews used in this project
│   ├── summarized_review.csv          # Summarized reviews data for content-based filtering
│   └── vectorized_data.csv            # Vectorized review data used in model input
│
├── mds-learning-material/             # Supporting materials used during development
│
├── recommender_new.ipynb              # Jupyter notebook containing all model experiments and development
├── .gitignore                         # Git ignore file
├── LICENSE                            # License information
└── README.md                          # Project documentation (this file)
```

---

## Installation and Setup

### Prerequisites

- Python 3.8+
- Conda (Anaconda or Miniconda)
  
To set up the environment for this project, we are using a `environment.yml` file that lists all the necessary dependencies, including LLM-related libraries.

### Installation Steps

1. **Clone the repository**:

```bash
git clone https://github.com/yourusername/llm-recommender-system.git
cd llm-recommender-system
```

2. **Create the environment using the YAML file**:

```bash
conda env create -f environment.yml
```

3. **Activate the environment**:

```bash
conda activate recommender_system
```

4. **Download the dataset** from Kaggle and place it in the `data/` folder.

5. **Run the Jupyter notebook**:

```bash
jupyter notebook recommender_new.ipynb
```

---

## Model Development

### 1. **Collaborative Filtering**

We applied collaborative filtering to predict a user’s rating of a book based on the ratings from similar users. Matrix factorization techniques, like Singular Value Decomposition (SVD), were used to identify latent factors.

**Key Metrics**:
- RMSE: **0.94**

### 2. **Content-Based Filtering with LLMs**

The content-based filtering model uses review text embeddings generated by LLMs (such as BERT). These embeddings capture semantic relationships between books, allowing for more contextually relevant recommendations.

**Key Metrics**:
- RMSE: **0.69**

### 3. **Hybrid Model**

Combining collaborative filtering with content-based filtering results in a hybrid recommendation system, providing a balance between user behavior and content similarity.

---

## Usage

You can explore the `recommender_new.ipynb` notebook, which covers:
- **Data exploration and preprocessing**
- **Collaborative filtering implementation**
- **LLM-based content filtering**
- **Combining models for hybrid recommendations**

The notebook provides code and explanations for how to replicate the model development process.

---

## Environment Configuration (environment.yml)

The project environment is defined in `environment.yml` and includes the following key dependencies:

```yaml
name: recommender_system
channels:
  - defaults
  - conda-forge
dependencies:
  - python=3.8
  - pandas
  - numpy
  - scikit-learn
  - jupyterlab
  - pip
  - pip:
    - transformers  # for LLMs like BERT
```

### Installing the Environment

To set up the project environment, use:

```bash
conda env create -f environment.yml
```

This will ensure all the required dependencies are installed, including those related to LLMs, making it easy to replicate the environment on other machines.

---

## License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for more details.

---

## Contributors

- **Mohammad Norouzi** - [GitHub Profile](https://github.com/MoNorouzi23)
- **Atabak Alishiri** - [GitHub Profile](https://github.com/atabak-alishiri)


Feel free to contribute by submitting a pull request or opening an issue!
