{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eec1218-5ab8-4639-8adf-b5a969693d9d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "## Introduction\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dbf6a5c-6590-4378-bdb9-b3c912dbf0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from hashlib import sha1\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from surprise import SVD\n",
    "from surprise import Dataset, Reader\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise import accuracy\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b961be-5f3c-4426-ab30-15b14a53cacc",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "## Data Description<a name=\"2\"></a>\n",
    "Given the large size of the dataset, only 10000 rows of the dataset is used for the models.\n",
    "This project utilizes a comprehensive dataset sourced from Kaggle, which can be accessed via the following link: (https://www.kaggle.com/datasets/beaglelee/amazon-reviews-us-books-v1-02-tsv-zip). The dataset consists of 15 columns and encompasses a substantial total of 3,105,370 rows, providing rich insights into customer feedback and product ratings specifically within the book category.\n",
    "\n",
    "Due to the extensive size of the dataset, a subset of 10,000 rows has been selected for analysis and modeling. This reduction allows for efficient processing while still capturing the diverse range of reviews and ratings present in the original dataset.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "7bbe7833-717b-4df6-8ea4-8f3b2f147ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "data = pd.read_csv(\"data/amazon_reviews_us_Books_v1_02.tsv\", sep='\\t', on_bad_lines='skip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "bf3228b3-7f08-4f8e-a746-7abd2473b066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>marketplace</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_category</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>vine</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>12076615</td>\n",
       "      <td>RQ58W7SMO911M</td>\n",
       "      <td>0385730586</td>\n",
       "      <td>122662979</td>\n",
       "      <td>Sisterhood of the Traveling Pants (Book 1)</td>\n",
       "      <td>Books</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>this book was a great learning novel!</td>\n",
       "      <td>this boook was a great one that you could lear...</td>\n",
       "      <td>2005-10-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>12703090</td>\n",
       "      <td>RF6IUKMGL8SF</td>\n",
       "      <td>0811828964</td>\n",
       "      <td>56191234</td>\n",
       "      <td>The Bad Girl's Guide to Getting What You Want</td>\n",
       "      <td>Books</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Fun Fluff</td>\n",
       "      <td>If you are looking for something to stimulate ...</td>\n",
       "      <td>2005-10-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>12257412</td>\n",
       "      <td>R1DOSHH6AI622S</td>\n",
       "      <td>1844161560</td>\n",
       "      <td>253182049</td>\n",
       "      <td>Eisenhorn (A Warhammer 40,000 Omnibus)</td>\n",
       "      <td>Books</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>this isn't a review</td>\n",
       "      <td>never read it-a young relative idicated he lik...</td>\n",
       "      <td>2005-10-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US</td>\n",
       "      <td>50732546</td>\n",
       "      <td>RATOTLA3OF70O</td>\n",
       "      <td>0373836635</td>\n",
       "      <td>348672532</td>\n",
       "      <td>Colby Conspiracy (Colby Agency)</td>\n",
       "      <td>Books</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>fine author on her A-game</td>\n",
       "      <td>Though she is honored to be Chicago Woman of t...</td>\n",
       "      <td>2005-10-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US</td>\n",
       "      <td>51964897</td>\n",
       "      <td>R1TNWRKIVHVYOV</td>\n",
       "      <td>0262181533</td>\n",
       "      <td>598678717</td>\n",
       "      <td>The Psychology of Proof: Deductive Reasoning i...</td>\n",
       "      <td>Books</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Execellent cursor examination</td>\n",
       "      <td>Review based on a cursory examination by Unive...</td>\n",
       "      <td>2005-10-14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  marketplace  customer_id       review_id  product_id  product_parent  \\\n",
       "0          US     12076615   RQ58W7SMO911M  0385730586       122662979   \n",
       "1          US     12703090    RF6IUKMGL8SF  0811828964        56191234   \n",
       "2          US     12257412  R1DOSHH6AI622S  1844161560       253182049   \n",
       "3          US     50732546   RATOTLA3OF70O  0373836635       348672532   \n",
       "4          US     51964897  R1TNWRKIVHVYOV  0262181533       598678717   \n",
       "\n",
       "                                       product_title product_category  \\\n",
       "0         Sisterhood of the Traveling Pants (Book 1)            Books   \n",
       "1      The Bad Girl's Guide to Getting What You Want            Books   \n",
       "2             Eisenhorn (A Warhammer 40,000 Omnibus)            Books   \n",
       "3                    Colby Conspiracy (Colby Agency)            Books   \n",
       "4  The Psychology of Proof: Deductive Reasoning i...            Books   \n",
       "\n",
       "   star_rating  helpful_votes  total_votes vine verified_purchase  \\\n",
       "0          4.0            2.0          3.0    N                 N   \n",
       "1          3.0            5.0          5.0    N                 N   \n",
       "2          4.0            1.0         22.0    N                 N   \n",
       "3          5.0            2.0          2.0    N                 N   \n",
       "4          4.0            0.0          2.0    N                 N   \n",
       "\n",
       "                         review_headline  \\\n",
       "0  this book was a great learning novel!   \n",
       "1                              Fun Fluff   \n",
       "2                    this isn't a review   \n",
       "3              fine author on her A-game   \n",
       "4          Execellent cursor examination   \n",
       "\n",
       "                                         review_body review_date  \n",
       "0  this boook was a great one that you could lear...  2005-10-14  \n",
       "1  If you are looking for something to stimulate ...  2005-10-14  \n",
       "2  never read it-a young relative idicated he lik...  2005-10-14  \n",
       "3  Though she is honored to be Chicago Woman of t...  2005-10-14  \n",
       "4  Review based on a cursory examination by Unive...  2005-10-14  "
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5e2f95-f813-4cd3-879c-34b3d3f9cfc1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "## Exploratory Data Analysis(EDA) <a name=\"3\"></a>\n",
    "\n",
    "This section describes the exploratory data analysis (EDA) techniques employed to derive valuable insights from the dataset, which will inform the subsequent stages of model development.\n",
    "\n",
    "To create a targeted subset for analysis, we identified product IDs and customer IDs associated with at least 100 reviews. This filtering process resulted in a dataset containing 24,466 rows, representing customer reviews. Our final subset includes 1,672 distinct products and 1,230 distinct customers, ensuring a diverse representation of both products and customers. This comprehensive approach enables us to conduct a thorough examination of customer feedback, facilitating deeper insights into their preferences and behaviors.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "bf4c5e63-e457-4f53-ba53-3b7d23e0686e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3105370 entries, 0 to 3105369\n",
      "Data columns (total 15 columns):\n",
      " #   Column             Dtype  \n",
      "---  ------             -----  \n",
      " 0   marketplace        object \n",
      " 1   customer_id        int64  \n",
      " 2   review_id          object \n",
      " 3   product_id         object \n",
      " 4   product_parent     int64  \n",
      " 5   product_title      object \n",
      " 6   product_category   object \n",
      " 7   star_rating        float64\n",
      " 8   helpful_votes      float64\n",
      " 9   total_votes        float64\n",
      " 10  vine               object \n",
      " 11  verified_purchase  object \n",
      " 12  review_headline    object \n",
      " 13  review_body        object \n",
      " 14  review_date        object \n",
      "dtypes: float64(3), int64(2), object(10)\n",
      "memory usage: 355.4+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "bcb0bdf6-8c2d-409f-9640-e625aefc7f38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "marketplace            0\n",
       "customer_id            0\n",
       "review_id              0\n",
       "product_id             0\n",
       "product_parent         0\n",
       "product_title          0\n",
       "product_category       0\n",
       "star_rating            4\n",
       "helpful_votes          4\n",
       "total_votes            4\n",
       "vine                   4\n",
       "verified_purchase      4\n",
       "review_headline       57\n",
       "review_body            4\n",
       "review_date          133\n",
       "dtype: int64"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "ed0a5f88-14af-4fd4-baf2-b22a8cd3c122",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "bd566ae0-7c36-40be-b983-4456e5c79188",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.replace(['null', 'N/A', '', ' '], np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "f8c3e030-ec22-4e02-9572-303fde59d229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "marketplace          0\n",
       "customer_id          0\n",
       "review_id            0\n",
       "product_id           0\n",
       "product_parent       0\n",
       "product_title        0\n",
       "product_category     0\n",
       "star_rating          0\n",
       "helpful_votes        0\n",
       "total_votes          0\n",
       "vine                 0\n",
       "verified_purchase    0\n",
       "review_headline      0\n",
       "review_body          0\n",
       "review_date          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "56ec071b-1471-4a84-adbf-8353cfa1f418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "marketplace                1\n",
       "customer_id          1502265\n",
       "review_id            3105184\n",
       "product_id            779692\n",
       "product_parent        666003\n",
       "product_title         713665\n",
       "product_category           1\n",
       "star_rating                5\n",
       "helpful_votes            942\n",
       "total_votes             1024\n",
       "vine                       2\n",
       "verified_purchase          2\n",
       "review_headline      2456998\n",
       "review_body          3070458\n",
       "review_date             3575\n",
       "dtype: int64"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "ee6755d9-ce24-4a8d-9a66-c8f580c147d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24466, 15)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selected a subset with customers and products with at least 100 reviews\n",
    "# Step 1: Filter customers with at least 100 reviews\n",
    "customer_review_counts = data.groupby('customer_id').size().reset_index(name='review_count')\n",
    "customers_with_at_least_100_reviews = customer_review_counts[customer_review_counts['review_count'] >= 100]\n",
    "\n",
    "# Step 2: Filter products with at least 100 reviews\n",
    "product_review_counts = data.groupby('product_id').size().reset_index(name='review_count')\n",
    "products_with_at_least_100_reviews = product_review_counts[product_review_counts['review_count'] >= 100]\n",
    "\n",
    "# Step 3: Filter the original dataset to only include customers and products with at least 100 reviews\n",
    "filtered_data = data[\n",
    "    (data['customer_id'].isin(customers_with_at_least_100_reviews['customer_id'])) &\n",
    "    (data['product_id'].isin(products_with_at_least_100_reviews['product_id']))\n",
    "]\n",
    "filtered_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "134c1fee-9f44-4bac-a943-4e4573faea09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>marketplace</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_category</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>vine</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>US</td>\n",
       "      <td>50230169</td>\n",
       "      <td>R23MCAR8GSV3T0</td>\n",
       "      <td>0451526341</td>\n",
       "      <td>380925201</td>\n",
       "      <td>Animal farm: A Fairy Story</td>\n",
       "      <td>Books</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Simple Yet Profound</td>\n",
       "      <td>A generation ago, the sight of the cover of Ge...</td>\n",
       "      <td>2005-10-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>US</td>\n",
       "      <td>50776149</td>\n",
       "      <td>RUCZYTA3MP0MR</td>\n",
       "      <td>038551428X</td>\n",
       "      <td>970964974</td>\n",
       "      <td>The Traveler (Fourth Realm Trilogy, Book 1)</td>\n",
       "      <td>Books</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Great Marketing for a Pretty Good Book</td>\n",
       "      <td>The most interesting thing about this book is ...</td>\n",
       "      <td>2005-10-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>US</td>\n",
       "      <td>12598621</td>\n",
       "      <td>RCL2ARHKWH6RL</td>\n",
       "      <td>059035342X</td>\n",
       "      <td>667539744</td>\n",
       "      <td>Harry Potter and the Sorcerer's Stone</td>\n",
       "      <td>Books</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>I Think Part Of The Charm Is You Feel Like You...</td>\n",
       "      <td>Even though this is the shortest book in the H...</td>\n",
       "      <td>2005-10-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>US</td>\n",
       "      <td>49770667</td>\n",
       "      <td>R2P4B3STC980QP</td>\n",
       "      <td>1594480001</td>\n",
       "      <td>659516630</td>\n",
       "      <td>The Kite Runner</td>\n",
       "      <td>Books</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Praiseworthy first novel</td>\n",
       "      <td>Well I thoroughly enjoyed this book. Although ...</td>\n",
       "      <td>2005-10-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>US</td>\n",
       "      <td>49828549</td>\n",
       "      <td>RM0CSYVWKHW5W</td>\n",
       "      <td>0671027360</td>\n",
       "      <td>141370518</td>\n",
       "      <td>Angels &amp; Demons</td>\n",
       "      <td>Books</td>\n",
       "      <td>1.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Preposterous</td>\n",
       "      <td>Early in this novel, our hero finds out that a...</td>\n",
       "      <td>2005-10-14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    marketplace  customer_id       review_id  product_id  product_parent  \\\n",
       "295          US     50230169  R23MCAR8GSV3T0  0451526341       380925201   \n",
       "307          US     50776149   RUCZYTA3MP0MR  038551428X       970964974   \n",
       "314          US     12598621   RCL2ARHKWH6RL  059035342X       667539744   \n",
       "363          US     49770667  R2P4B3STC980QP  1594480001       659516630   \n",
       "406          US     49828549   RM0CSYVWKHW5W  0671027360       141370518   \n",
       "\n",
       "                                   product_title product_category  \\\n",
       "295                   Animal farm: A Fairy Story            Books   \n",
       "307  The Traveler (Fourth Realm Trilogy, Book 1)            Books   \n",
       "314        Harry Potter and the Sorcerer's Stone            Books   \n",
       "363                              The Kite Runner            Books   \n",
       "406                              Angels & Demons            Books   \n",
       "\n",
       "     star_rating  helpful_votes  total_votes vine verified_purchase  \\\n",
       "295          4.0            2.0          2.0    N                 N   \n",
       "307          5.0            2.0          5.0    N                 N   \n",
       "314          5.0            2.0          2.0    N                 N   \n",
       "363          5.0            4.0          4.0    N                 N   \n",
       "406          1.0           31.0         39.0    N                 N   \n",
       "\n",
       "                                       review_headline  \\\n",
       "295                                Simple Yet Profound   \n",
       "307             Great Marketing for a Pretty Good Book   \n",
       "314  I Think Part Of The Charm Is You Feel Like You...   \n",
       "363                           Praiseworthy first novel   \n",
       "406                                       Preposterous   \n",
       "\n",
       "                                           review_body review_date  \n",
       "295  A generation ago, the sight of the cover of Ge...  2005-10-14  \n",
       "307  The most interesting thing about this book is ...  2005-10-14  \n",
       "314  Even though this is the shortest book in the H...  2005-10-14  \n",
       "363  Well I thoroughly enjoyed this book. Although ...  2005-10-14  \n",
       "406  Early in this novel, our hero finds out that a...  2005-10-14  "
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "64c9e215-042b-45e3-be92-6d84b8400d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data.to_csv('data/amazon_reviews_subset_100.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "f3c23737-7b42-4942-8e8e-7b819d3d22a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "marketplace              1\n",
       "customer_id           1230\n",
       "review_id            24466\n",
       "product_id            1672\n",
       "product_parent        1485\n",
       "product_title         1562\n",
       "product_category         1\n",
       "star_rating              5\n",
       "helpful_votes          423\n",
       "total_votes            460\n",
       "vine                     1\n",
       "verified_purchase        2\n",
       "review_headline      23305\n",
       "review_body          24314\n",
       "review_date           2574\n",
       "dtype: int64"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961a5e36-f4fd-45b6-8c43-6740dc262019",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "## Collaborative Filtering\n",
    "**Collaborative Filtering** is a widely-used technique for addressing the challenge of missing entries in a utility matrix, leveraging user behavior and interactions to make recommendations. This approach operates on the principle that users who have agreed in the past will continue to agree in the future, allowing the model to infer preferences based on the preferences of similar users.\n",
    "\n",
    "This method can be likened to advanced dimensionality reduction techniques such as Latent Semantic Analysis (LSA) or Truncated Singular Value Decomposition (SVD). By capturing the underlying relationships between users and items, collaborative filtering helps to predict missing values, enhancing the accuracy and relevance of recommendations.\n",
    "\n",
    "In this project, we will implement collaborative filtering as our baseline model to improve user experience by personalizing content based on historical data, thus enabling more informed decision-making.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "491d78cb-beef-48db-88bc-ddfb95474efd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>star_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50230169</td>\n",
       "      <td>0451526341</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50776149</td>\n",
       "      <td>038551428X</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12598621</td>\n",
       "      <td>059035342X</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49770667</td>\n",
       "      <td>1594480001</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49828549</td>\n",
       "      <td>0671027360</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  product_id  star_rating\n",
       "0     50230169  0451526341          4.0\n",
       "1     50776149  038551428X          5.0\n",
       "2     12598621  059035342X          5.0\n",
       "3     49770667  1594480001          5.0\n",
       "4     49828549  0671027360          1.0"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the data\n",
    "coll_data = filtered_data[['customer_id', 'product_id', 'star_rating']].reset_index(drop=True)\n",
    "coll_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "88259219-0172-4596-bfea-9f985d53d74e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "marketplace              1\n",
       "customer_id           1230\n",
       "review_id            24466\n",
       "product_id            1672\n",
       "product_parent        1485\n",
       "product_title         1562\n",
       "product_category         1\n",
       "star_rating              5\n",
       "helpful_votes          423\n",
       "total_votes            460\n",
       "vine                     1\n",
       "verified_purchase        2\n",
       "review_headline      23305\n",
       "review_body          24314\n",
       "review_date           2574\n",
       "dtype: int64"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "ad78779c-6dbe-4255-a4f8-efe18c29515a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customer_id    1230\n",
       "product_id     1672\n",
       "star_rating       5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coll_data.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "742f1968-46bc-490b-b7be-6a68094e4ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of customers (N)  : 1230\n",
      "Number of products (M) : 1672\n"
     ]
    }
   ],
   "source": [
    "# Number of customers and products\n",
    "user_key = \"customer_id\"\n",
    "item_key = \"product_id\"\n",
    "N = len(np.unique(coll_data[user_key])) \n",
    "M = len(np.unique(coll_data[item_key]))\n",
    "print(f\"Number of customers (N)  : {N}\")\n",
    "print(f\"Number of products (M) : {M}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "070d7eb1-ed77-4bba-b44b-f8fd9a0086dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-nan ratings percentage: 1.19\n"
     ]
    }
   ],
   "source": [
    "non_nan_ratings_percentage = (len(coll_data) / (N * M) * 100) \n",
    "print(f\"Non-nan ratings percentage: {np.round(non_nan_ratings_percentage,3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3ca5bd28-648d-4812-923c-1b97f4b711fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of ratings per customer : 19.89\n",
      "Average number of ratings per product: 14.63\n"
     ]
    }
   ],
   "source": [
    "avg_nratings_per_user = coll_data.groupby(user_key).size().mean()\n",
    "avg_nratings_per_movie = coll_data.groupby(item_key).size().mean()\n",
    "print(f\"Average number of ratings per customer : {avg_nratings_per_user:.2f}\")\n",
    "print(f\"Average number of ratings per product: {avg_nratings_per_movie:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "01a72bc1-78c6-4ead-b28b-abb0a17873a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Splitting\n",
    "X = coll_data.copy()\n",
    "y = coll_data['customer_id']\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "b70bb14a-b22d-4283-9d34-67b59d96b3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_mapper = dict(zip(np.unique(coll_data[user_key]), list(range(N))))\n",
    "item_mapper = dict(zip(np.unique(coll_data[item_key]), list(range(M))))\n",
    "user_inverse_mapper = dict(zip(list(range(N)), np.unique(coll_data[user_key])))\n",
    "item_inverse_mapper = dict(zip(list(range(M)), np.unique(coll_data[item_key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "c030f351-d414-410f-a4e2-be5ae058f3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_Y_from_ratings(data, N, M):\n",
    "    Y = np.zeros((N, M))\n",
    "    Y.fill(np.nan)\n",
    "    for index, val in data.iterrows():\n",
    "        n = user_mapper[val[user_key]]\n",
    "        m = item_mapper[val[item_key]]\n",
    "        Y[n, m] = val[\"star_rating\"]\n",
    "\n",
    "    return Y\n",
    "\n",
    "train_mat = create_Y_from_ratings(X_train, N, M)\n",
    "valid_mat = create_Y_from_ratings(X_valid, N, M)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "0446201b-ba26-4bdb-8006-f9852b0d56e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of non-nan elements in train_mat: 19085\n",
      "Number of non-nan elements in valid_mat: 4855\n"
     ]
    }
   ],
   "source": [
    "# What's the number of non-nan elements in train_mat (nnn_train_mat)?\n",
    "nnn_train_mat = np.sum(~np.isnan(train_mat)) \n",
    "\n",
    "# What's the number of non-nan elements in valid_mat (nnn_valid_mat)?\n",
    "nnn_valid_mat = np.sum(~np.isnan(valid_mat)) \n",
    "print(f\"Number of non-nan elements in train_mat: {nnn_train_mat}\")\n",
    "print(f\"Number of non-nan elements in valid_mat: {nnn_valid_mat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "329bf88f-8133-49e8-823a-a03c2e5022c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "def error(Y1, Y2):\n",
    "    \"\"\"\n",
    "    Given two matrices of the same shape, \n",
    "    returns the root mean squared error (RMSE).\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.nanmean((Y1 - Y2) ** 2))\n",
    "\n",
    "\n",
    "def evaluate(pred_Y, train_mat, valid_mat, model_name=\"Global average\"):\n",
    "    \"\"\"\n",
    "    Given predicted utility matrix and train and validation utility matrices \n",
    "    print train and validation RMSEs.\n",
    "    \"\"\"\n",
    "    print(\"%s train RMSE: %0.2f\" % (model_name, error(pred_Y, train_mat)))\n",
    "    print(\"%s valid RMSE: %0.2f\" % (model_name, error(pred_Y, valid_mat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "0a2c14fa-0053-4ff2-b3a5-6fd4aa05b08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global average train RMSE: 1.06\n",
      "Global average valid RMSE: 1.09\n"
     ]
    }
   ],
   "source": [
    "# global average rating baseline\n",
    "avg = np.nanmean(train_mat)\n",
    "pred_g = np.zeros(train_mat.shape) + avg\n",
    "evaluate(pred_g, train_mat, valid_mat, model_name=\"Global average\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "bfc44586-a17a-4aef-864d-a370f729bf5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-user average train RMSE: 0.94\n",
      "Per-user average valid RMSE: 1.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j6/44dd1c8s33xg2kn35nxq3f5m0000gn/T/ipykernel_48949/2152599090.py:2: RuntimeWarning: Mean of empty slice\n",
      "  avg_n = np.nanmean(train_mat, axis=1)\n"
     ]
    }
   ],
   "source": [
    "# Per-user average baseline\n",
    "avg_n = np.nanmean(train_mat, axis=1)\n",
    "avg_n[\n",
    "    np.isnan(avg_n)\n",
    "] = avg  \n",
    "pred_n = np.tile(avg_n[:, None], (1, M))\n",
    "evaluate(pred_n, train_mat, valid_mat, model_name=\"Per-user average\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "681a3704-cc72-433d-a3d1-83c1844c1563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-product average train RMSE: 0.93\n",
      "Per-product average valid RMSE: 1.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j6/44dd1c8s33xg2kn35nxq3f5m0000gn/T/ipykernel_48949/1388612677.py:2: RuntimeWarning: Mean of empty slice\n",
      "  avg_m = np.nanmean(train_mat, axis=0)\n"
     ]
    }
   ],
   "source": [
    "# Per-product average baseline\n",
    "avg_m = np.nanmean(train_mat, axis=0)\n",
    "avg_m[np.isnan(avg_m)] = avg\n",
    "pred_m = np.tile(avg_m[None, :], (N, 1))\n",
    "evaluate(pred_m, train_mat, valid_mat, model_name=\"Per-product average\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "ff5925cb-82d4-40de-ac29-dfdc496ceafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-user and product average train RMSE: 0.88\n",
      "Per-user and product average valid RMSE: 0.99\n"
     ]
    }
   ],
   "source": [
    "# Average of per-user and per-product average baselines\n",
    "pred_n_m = (pred_n + pred_m) * 0.5\n",
    "evaluate(pred_n_m, train_mat, valid_mat, model_name=\"Per-user and product average\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "705bb771-a661-4c89-9c40-909caf601848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of neighbours:  10\n",
      "Global average train RMSE: 0.00\n",
      "Global average valid RMSE: 1.08\n",
      "\n",
      "Number of neighbours:  15\n",
      "Global average train RMSE: 0.00\n",
      "Global average valid RMSE: 1.08\n",
      "\n",
      "Number of neighbours:  18\n",
      "Global average train RMSE: 0.00\n",
      "Global average valid RMSE: 1.08\n",
      "\n",
      "Number of neighbours:  20\n",
      "Global average train RMSE: 0.00\n",
      "Global average valid RMSE: 1.08\n",
      "\n",
      "Number of neighbours:  40\n",
      "Global average train RMSE: 0.00\n",
      "Global average valid RMSE: 1.08\n"
     ]
    }
   ],
   "source": [
    "# K-nearest neighbours imputation\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "num_neighs = [10, 15, 18, 20, 40]\n",
    "for n_neighbors in num_neighs:\n",
    "    print(\"\\nNumber of neighbours: \", n_neighbors)\n",
    "    imputer = KNNImputer(n_neighbors=n_neighbors, keep_empty_features=True)\n",
    "    pred_knn = imputer.fit_transform(train_mat)\n",
    "    evaluate(pred_knn, train_mat, valid_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "3294ab37-5558-4fed-ab30-8f9d1aac7ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "TruncatedSVD (k = 10) train RMSE: 0.82\n",
      "TruncatedSVD (k = 10) valid RMSE: 0.98\n",
      "\n",
      "\n",
      "TruncatedSVD (k = 50) train RMSE: 0.68\n",
      "TruncatedSVD (k = 50) valid RMSE: 0.98\n",
      "\n",
      "\n",
      "TruncatedSVD (k = 100) train RMSE: 0.56\n",
      "TruncatedSVD (k = 100) valid RMSE: 0.97\n",
      "\n",
      "\n",
      "TruncatedSVD (k = 200) train RMSE: 0.40\n",
      "TruncatedSVD (k = 200) valid RMSE: 0.97\n",
      "\n",
      "\n",
      "TruncatedSVD (k = 500) train RMSE: 0.15\n",
      "TruncatedSVD (k = 500) valid RMSE: 0.97\n",
      "\n",
      "\n",
      "TruncatedSVD (k = 1000) train RMSE: 0.01\n",
      "TruncatedSVD (k = 1000) valid RMSE: 0.97\n"
     ]
    }
   ],
   "source": [
    "# collaborative filtering with TruncatedSVD()\n",
    "def reconstruct_svd(Z, W, avg_n, avg_m):\n",
    "    return Z @ W + 0.5 * avg_n[:, None] + 0.5 * avg_m[None]\n",
    "\n",
    "\n",
    "train_mat_svd = train_mat - 0.5 * avg_n[:, None] - 0.5 * avg_m[None]\n",
    "train_mat_svd = np.nan_to_num(train_mat_svd)\n",
    "\n",
    "k_range = [10, 50, 100, 200, 500, 1000]\n",
    "for k in k_range:\n",
    "    print(\"\\n\")\n",
    "    tsvd = TruncatedSVD(n_components=k)\n",
    "    Z = tsvd.fit_transform(train_mat_svd)\n",
    "    W = tsvd.components_\n",
    "    X_hat = reconstruct_svd(Z, W, avg_n, avg_m)\n",
    "    evaluate(X_hat, train_mat, valid_mat, model_name=\"TruncatedSVD (k = %d)\" % k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "e24b2cfc-a3ce-43ea-927c-750370cab260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using surprise package\n",
    "reader = Reader()\n",
    "data = Dataset.load_from_df(coll_data, reader)  \n",
    "\n",
    "k = 10\n",
    "algo = SVD(n_factors=k, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "869fb9dd-5ae1-4a3b-b638-f34bb774cf7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RMSE of algorithm SVD on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    0.9642  0.9487  0.9521  0.9302  0.9462  0.9483  0.0109  \n",
      "Fit time          0.03    0.02    0.02    0.02    0.02    0.02    0.00    \n",
      "Test time         0.01    0.01    0.01    0.01    0.01    0.01    0.00    \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_rmse</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>test_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.964154</td>\n",
       "      <td>0.025022</td>\n",
       "      <td>0.009257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.948651</td>\n",
       "      <td>0.023375</td>\n",
       "      <td>0.008167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.952118</td>\n",
       "      <td>0.022660</td>\n",
       "      <td>0.008023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.930238</td>\n",
       "      <td>0.022565</td>\n",
       "      <td>0.007989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.946221</td>\n",
       "      <td>0.022534</td>\n",
       "      <td>0.007865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_rmse  fit_time  test_time\n",
       "0   0.964154  0.025022   0.009257\n",
       "1   0.948651  0.023375   0.008167\n",
       "2   0.952118  0.022660   0.008023\n",
       "3   0.930238  0.022565   0.007989\n",
       "4   0.946221  0.022534   0.007865"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(cross_validate(algo, data, measures=[\"RMSE\"], cv=5, verbose=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fdfa40-c5a4-4ee7-8b63-9cb1c031c7e4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "## Incorporating Reviews\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "1c5663cd-1d87-40b4-bcce-33393f8206e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50230169</td>\n",
       "      <td>0451526341</td>\n",
       "      <td>4.0</td>\n",
       "      <td>A generation ago, the sight of the cover of Ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50776149</td>\n",
       "      <td>038551428X</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The most interesting thing about this book is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12598621</td>\n",
       "      <td>059035342X</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Even though this is the shortest book in the H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49770667</td>\n",
       "      <td>1594480001</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Well I thoroughly enjoyed this book. Although ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49828549</td>\n",
       "      <td>0671027360</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Early in this novel, our hero finds out that a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  product_id  star_rating  \\\n",
       "0     50230169  0451526341          4.0   \n",
       "1     50776149  038551428X          5.0   \n",
       "2     12598621  059035342X          5.0   \n",
       "3     49770667  1594480001          5.0   \n",
       "4     49828549  0671027360          1.0   \n",
       "\n",
       "                                         review_body  \n",
       "0  A generation ago, the sight of the cover of Ge...  \n",
       "1  The most interesting thing about this book is ...  \n",
       "2  Even though this is the shortest book in the H...  \n",
       "3  Well I thoroughly enjoyed this book. Although ...  \n",
       "4  Early in this novel, our hero finds out that a...  "
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data = filtered_data[['customer_id', 'product_id', 'star_rating', 'review_body']].reset_index(drop=True)\n",
    "review_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "91118f4e-6303-4938-b649-46fc7e62fcf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24466, 4)"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "ddadfffb-02c4-438f-bc62-2f0500356f0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customer_id     1230\n",
       "product_id      1672\n",
       "star_rating        5\n",
       "review_body    24314\n",
       "dtype: int64"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a9d0ce39-eb3e-403a-bc96-1063b6b9cb5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>aggregated_reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0020425651</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>susan cooper dark rising sequence joined pryda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0028610105</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>sheer diversity recipe japanese thai indian fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>006001203X</td>\n",
       "      <td>4.100000</td>\n",
       "      <td>health care proffesional tell way traumatising...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0060096195</td>\n",
       "      <td>4.428571</td>\n",
       "      <td>started reading one bathtub get id gotten fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>006016848X</td>\n",
       "      <td>3.562500</td>\n",
       "      <td>really like book time everyone want equality s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id  average_rating  \\\n",
       "0  0020425651        5.000000   \n",
       "1  0028610105        4.400000   \n",
       "2  006001203X        4.100000   \n",
       "3  0060096195        4.428571   \n",
       "4  006016848X        3.562500   \n",
       "\n",
       "                                  aggregated_reviews  \n",
       "0  susan cooper dark rising sequence joined pryda...  \n",
       "1  sheer diversity recipe japanese thai indian fr...  \n",
       "2  health care proffesional tell way traumatising...  \n",
       "3  started reading one bathtub get id gotten fina...  \n",
       "4  really like book time everyone want equality s...  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean the reviews\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# Initialize stopwords and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Remove stopwords and lemmatize\n",
    "    text = ' '.join(lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words)\n",
    "    return text\n",
    "# Clean the 'review_body' column\n",
    "review_data['cleaned_review_body'] = review_data['review_body'].apply(clean_text)\n",
    "\n",
    "# Step 1: Group by 'customer_id' and 'product_id' and aggregate\n",
    "aggregated_data = review_data.groupby(['product_id']).agg(\n",
    "    average_rating=('star_rating', 'mean'),         # Mean of the star ratings\n",
    "    aggregated_reviews=('cleaned_review_body', ' '.join)  # Concatenate all cleaned review bodies\n",
    ").reset_index()\n",
    "\n",
    "# Display the aggregated DataFrame\n",
    "aggregated_data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3f968123-0cdb-4d99-8c46-97d5cc6f3cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "product_id            1672\n",
       "average_rating         425\n",
       "aggregated_reviews    1672\n",
       "summarized_reviews    1669\n",
       "dtype: int64"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_data.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ab2ae5b9-9913-44d1-84f1-3b67840ffbd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1672, 3)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "546a89ea-eb08-4054-ac6d-30e4898c5b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "# Load the BART model and tokenizer\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "58c0d1cd-207f-453d-83a5-7d5378727c2a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['customer_id'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m aggregated_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummarized_reviews\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m summarize_reviews(aggregated_data)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Display the result\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[43maggregated_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcustomer_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mproduct_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maverage_rating\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msummarized_reviews\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/563/lib/python3.11/site-packages/pandas/core/frame.py:4096\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4094\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4095\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4096\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4098\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/563/lib/python3.11/site-packages/pandas/core/indexes/base.py:6199\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6196\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6197\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6199\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6201\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6203\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/563/lib/python3.11/site-packages/pandas/core/indexes/base.py:6251\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6250\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6251\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['customer_id'] not in index\""
     ]
    }
   ],
   "source": [
    "\n",
    "def summarize_reviews(df):\n",
    "    summaries = []\n",
    "    for review in df['aggregated_reviews']:\n",
    "        inputs = tokenizer(review, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "        summary_ids = model.generate(inputs[\"input_ids\"], max_length=50, num_beams=4, early_stopping=True)\n",
    "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        summaries.append(summary)\n",
    "    return summaries\n",
    "\n",
    "# Summarizing reviews \n",
    "aggregated_data['summarized_reviews'] = summarize_reviews(aggregated_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "78767ec1-849c-49ae-ab25-ed83e9a5bc7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>aggregated_reviews</th>\n",
       "      <th>summarized_reviews</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0020425651</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>susan cooper dark rising sequence joined pryda...</td>\n",
       "      <td>susan cooper dark rising sequence joined pryda...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0028610105</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>sheer diversity recipe japanese thai indian fr...</td>\n",
       "      <td>sheer diversity recipe japanese thai indian fr...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.975348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>006001203X</td>\n",
       "      <td>4.100000</td>\n",
       "      <td>health care proffesional tell way traumatising...</td>\n",
       "      <td>health care proffesional tell way traumatising...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.997175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0060096195</td>\n",
       "      <td>4.428571</td>\n",
       "      <td>started reading one bathtub get id gotten fina...</td>\n",
       "      <td>started reading one bathtub get id gotten fina...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.993411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>006016848X</td>\n",
       "      <td>3.562500</td>\n",
       "      <td>really like book time everyone want equality s...</td>\n",
       "      <td>really like book time everyone want equality s...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.977043</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id  average_rating  \\\n",
       "0  0020425651        5.000000   \n",
       "1  0028610105        4.400000   \n",
       "2  006001203X        4.100000   \n",
       "3  0060096195        4.428571   \n",
       "4  006016848X        3.562500   \n",
       "\n",
       "                                  aggregated_reviews  \\\n",
       "0  susan cooper dark rising sequence joined pryda...   \n",
       "1  sheer diversity recipe japanese thai indian fr...   \n",
       "2  health care proffesional tell way traumatising...   \n",
       "3  started reading one bathtub get id gotten fina...   \n",
       "4  really like book time everyone want equality s...   \n",
       "\n",
       "                                  summarized_reviews  sentiment  \\\n",
       "0  susan cooper dark rising sequence joined pryda...          1   \n",
       "1  sheer diversity recipe japanese thai indian fr...          1   \n",
       "2  health care proffesional tell way traumatising...         -1   \n",
       "3  started reading one bathtub get id gotten fina...          1   \n",
       "4  really like book time everyone want equality s...         -1   \n",
       "\n",
       "   sentiment_score  \n",
       "0         0.998871  \n",
       "1         0.975348  \n",
       "2         0.997175  \n",
       "3         0.993411  \n",
       "4         0.977043  "
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "d0a15928-db3d-40ca-9e50-1a5a84b855e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>summarized_reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0020425651</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>susan cooper dark rising sequence joined pryda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0028610105</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>sheer diversity recipe japanese thai indian fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>006001203X</td>\n",
       "      <td>4.100000</td>\n",
       "      <td>health care proffesional tell way traumatising...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0060096195</td>\n",
       "      <td>4.428571</td>\n",
       "      <td>started reading one bathtub get id gotten fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>006016848X</td>\n",
       "      <td>3.562500</td>\n",
       "      <td>really like book time everyone want equality s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1667</th>\n",
       "      <td>1931412065</td>\n",
       "      <td>4.875000</td>\n",
       "      <td>confirmed low carber year year constant raveno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1668</th>\n",
       "      <td>1931498717</td>\n",
       "      <td>4.727273</td>\n",
       "      <td>selection book group sunday september since or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1669</th>\n",
       "      <td>1931561648</td>\n",
       "      <td>4.437500</td>\n",
       "      <td>said time traveler wife nonconventional love s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1670</th>\n",
       "      <td>1931866007</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>book consists transcript interview mike litman...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1671</th>\n",
       "      <td>B0000WZWSI</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>first normally dont give many five star rating...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1672 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      product_id  average_rating  \\\n",
       "0     0020425651        5.000000   \n",
       "1     0028610105        4.400000   \n",
       "2     006001203X        4.100000   \n",
       "3     0060096195        4.428571   \n",
       "4     006016848X        3.562500   \n",
       "...          ...             ...   \n",
       "1667  1931412065        4.875000   \n",
       "1668  1931498717        4.727273   \n",
       "1669  1931561648        4.437500   \n",
       "1670  1931866007        5.000000   \n",
       "1671  B0000WZWSI        5.000000   \n",
       "\n",
       "                                     summarized_reviews  \n",
       "0     susan cooper dark rising sequence joined pryda...  \n",
       "1     sheer diversity recipe japanese thai indian fr...  \n",
       "2     health care proffesional tell way traumatising...  \n",
       "3     started reading one bathtub get id gotten fina...  \n",
       "4     really like book time everyone want equality s...  \n",
       "...                                                 ...  \n",
       "1667  confirmed low carber year year constant raveno...  \n",
       "1668  selection book group sunday september since or...  \n",
       "1669  said time traveler wife nonconventional love s...  \n",
       "1670  book consists transcript interview mike litman...  \n",
       "1671  first normally dont give many five star rating...  \n",
       "\n",
       "[1672 rows x 3 columns]"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_data[['product_id', 'average_rating', 'summarized_reviews']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "f5fb1915-1814-4f0e-8d0d-5358abaf719c",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_data.to_csv('data/summarized_review.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "582e645f-cc72-466a-8b84-88058830bed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data= aggregated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "28ecfc96-c663-4aa0-9493-cca3cbbf6609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>aggregated_reviews</th>\n",
       "      <th>summarized_reviews</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>...</th>\n",
       "      <th>374</th>\n",
       "      <th>375</th>\n",
       "      <th>376</th>\n",
       "      <th>377</th>\n",
       "      <th>378</th>\n",
       "      <th>379</th>\n",
       "      <th>380</th>\n",
       "      <th>381</th>\n",
       "      <th>382</th>\n",
       "      <th>383</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0020425651</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>susan cooper dark rising sequence joined pryda...</td>\n",
       "      <td>susan cooper dark rising sequence joined pryda...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998871</td>\n",
       "      <td>-0.070799</td>\n",
       "      <td>-0.061837</td>\n",
       "      <td>-0.003631</td>\n",
       "      <td>0.012133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.065155</td>\n",
       "      <td>0.053727</td>\n",
       "      <td>0.003597</td>\n",
       "      <td>0.088892</td>\n",
       "      <td>-0.042067</td>\n",
       "      <td>0.041044</td>\n",
       "      <td>0.070728</td>\n",
       "      <td>-0.043085</td>\n",
       "      <td>-0.064512</td>\n",
       "      <td>0.038242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0028610105</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>sheer diversity recipe japanese thai indian fr...</td>\n",
       "      <td>sheer diversity recipe japanese thai indian fr...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.975348</td>\n",
       "      <td>-0.073018</td>\n",
       "      <td>-0.023593</td>\n",
       "      <td>0.066772</td>\n",
       "      <td>0.036159</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042942</td>\n",
       "      <td>0.037886</td>\n",
       "      <td>-0.001067</td>\n",
       "      <td>-0.009074</td>\n",
       "      <td>0.065551</td>\n",
       "      <td>-0.054624</td>\n",
       "      <td>0.067726</td>\n",
       "      <td>0.079832</td>\n",
       "      <td>-0.015437</td>\n",
       "      <td>-0.041357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>006001203X</td>\n",
       "      <td>4.100000</td>\n",
       "      <td>health care proffesional tell way traumatising...</td>\n",
       "      <td>health care proffesional tell way traumatising...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.997175</td>\n",
       "      <td>0.004559</td>\n",
       "      <td>0.033951</td>\n",
       "      <td>0.020969</td>\n",
       "      <td>0.073957</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005109</td>\n",
       "      <td>0.098458</td>\n",
       "      <td>0.004446</td>\n",
       "      <td>0.010148</td>\n",
       "      <td>-0.050168</td>\n",
       "      <td>0.036669</td>\n",
       "      <td>0.133147</td>\n",
       "      <td>0.013290</td>\n",
       "      <td>0.063390</td>\n",
       "      <td>0.043042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0060096195</td>\n",
       "      <td>4.428571</td>\n",
       "      <td>started reading one bathtub get id gotten fina...</td>\n",
       "      <td>started reading one bathtub get id gotten fina...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.993411</td>\n",
       "      <td>-0.066638</td>\n",
       "      <td>-0.083743</td>\n",
       "      <td>0.053587</td>\n",
       "      <td>0.066727</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046725</td>\n",
       "      <td>0.029233</td>\n",
       "      <td>0.014859</td>\n",
       "      <td>0.084216</td>\n",
       "      <td>-0.085885</td>\n",
       "      <td>0.048461</td>\n",
       "      <td>0.023749</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>-0.075700</td>\n",
       "      <td>-0.034523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>006016848X</td>\n",
       "      <td>3.562500</td>\n",
       "      <td>really like book time everyone want equality s...</td>\n",
       "      <td>really like book time everyone want equality s...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.977043</td>\n",
       "      <td>-0.061388</td>\n",
       "      <td>0.048214</td>\n",
       "      <td>0.015069</td>\n",
       "      <td>-0.003492</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041296</td>\n",
       "      <td>0.018716</td>\n",
       "      <td>0.018985</td>\n",
       "      <td>0.015178</td>\n",
       "      <td>-0.019459</td>\n",
       "      <td>0.000856</td>\n",
       "      <td>0.134094</td>\n",
       "      <td>-0.078086</td>\n",
       "      <td>0.004868</td>\n",
       "      <td>-0.001025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 390 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id  average_rating  \\\n",
       "0  0020425651        5.000000   \n",
       "1  0028610105        4.400000   \n",
       "2  006001203X        4.100000   \n",
       "3  0060096195        4.428571   \n",
       "4  006016848X        3.562500   \n",
       "\n",
       "                                  aggregated_reviews  \\\n",
       "0  susan cooper dark rising sequence joined pryda...   \n",
       "1  sheer diversity recipe japanese thai indian fr...   \n",
       "2  health care proffesional tell way traumatising...   \n",
       "3  started reading one bathtub get id gotten fina...   \n",
       "4  really like book time everyone want equality s...   \n",
       "\n",
       "                                  summarized_reviews  sentiment  \\\n",
       "0  susan cooper dark rising sequence joined pryda...          1   \n",
       "1  sheer diversity recipe japanese thai indian fr...          1   \n",
       "2  health care proffesional tell way traumatising...         -1   \n",
       "3  started reading one bathtub get id gotten fina...          1   \n",
       "4  really like book time everyone want equality s...         -1   \n",
       "\n",
       "   sentiment_score         0         1         2         3  ...       374  \\\n",
       "0         0.998871 -0.070799 -0.061837 -0.003631  0.012133  ...  0.065155   \n",
       "1         0.975348 -0.073018 -0.023593  0.066772  0.036159  ...  0.042942   \n",
       "2         0.997175  0.004559  0.033951  0.020969  0.073957  ... -0.005109   \n",
       "3         0.993411 -0.066638 -0.083743  0.053587  0.066727  ...  0.046725   \n",
       "4         0.977043 -0.061388  0.048214  0.015069 -0.003492  ...  0.041296   \n",
       "\n",
       "        375       376       377       378       379       380       381  \\\n",
       "0  0.053727  0.003597  0.088892 -0.042067  0.041044  0.070728 -0.043085   \n",
       "1  0.037886 -0.001067 -0.009074  0.065551 -0.054624  0.067726  0.079832   \n",
       "2  0.098458  0.004446  0.010148 -0.050168  0.036669  0.133147  0.013290   \n",
       "3  0.029233  0.014859  0.084216 -0.085885  0.048461  0.023749  0.003057   \n",
       "4  0.018716  0.018985  0.015178 -0.019459  0.000856  0.134094 -0.078086   \n",
       "\n",
       "        382       383  \n",
       "0 -0.064512  0.038242  \n",
       "1 -0.015437 -0.041357  \n",
       "2  0.063390  0.043042  \n",
       "3 -0.075700 -0.034523  \n",
       "4  0.004868 -0.001025  \n",
       "\n",
       "[5 rows x 390 columns]"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load a pre-trained model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2') \n",
    "\n",
    "# Encode the summaries to get embeddings\n",
    "embeddings = model.encode(sample_data['summarized_reviews'].tolist())\n",
    "\n",
    "# Convert embeddings to a DataFrame\n",
    "embeddings_df = pd.DataFrame(embeddings)\n",
    "\n",
    "vectorized_data = pd.concat([sample_data, embeddings_df], axis=1)\n",
    "vectorized_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "0dcf4837-b69c-4cef-a11a-042480032f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>aggregated_reviews</th>\n",
       "      <th>summarized_reviews</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>...</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0020425651</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>susan cooper dark rising sequence joined pryda...</td>\n",
       "      <td>susan cooper dark rising sequence joined pryda...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998871</td>\n",
       "      <td>0.635252</td>\n",
       "      <td>-0.269041</td>\n",
       "      <td>0.012194</td>\n",
       "      <td>-0.010467</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.188243</td>\n",
       "      <td>-0.027312</td>\n",
       "      <td>-0.050258</td>\n",
       "      <td>0.066114</td>\n",
       "      <td>-0.039863</td>\n",
       "      <td>-0.032078</td>\n",
       "      <td>0.067193</td>\n",
       "      <td>-0.036900</td>\n",
       "      <td>0.138247</td>\n",
       "      <td>0.047895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0028610105</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>sheer diversity recipe japanese thai indian fr...</td>\n",
       "      <td>sheer diversity recipe japanese thai indian fr...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.975348</td>\n",
       "      <td>0.261072</td>\n",
       "      <td>0.044044</td>\n",
       "      <td>0.130149</td>\n",
       "      <td>0.080442</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.076873</td>\n",
       "      <td>-0.055025</td>\n",
       "      <td>0.061867</td>\n",
       "      <td>0.125944</td>\n",
       "      <td>0.289912</td>\n",
       "      <td>0.249448</td>\n",
       "      <td>0.005777</td>\n",
       "      <td>0.024197</td>\n",
       "      <td>0.114358</td>\n",
       "      <td>0.179739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>006001203X</td>\n",
       "      <td>4.100000</td>\n",
       "      <td>health care proffesional tell way traumatising...</td>\n",
       "      <td>health care proffesional tell way traumatising...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.997175</td>\n",
       "      <td>0.257239</td>\n",
       "      <td>0.295288</td>\n",
       "      <td>0.238396</td>\n",
       "      <td>0.178317</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015927</td>\n",
       "      <td>0.056299</td>\n",
       "      <td>0.268024</td>\n",
       "      <td>0.118565</td>\n",
       "      <td>0.062791</td>\n",
       "      <td>0.051532</td>\n",
       "      <td>0.155097</td>\n",
       "      <td>0.039039</td>\n",
       "      <td>-0.163372</td>\n",
       "      <td>0.166068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0060096195</td>\n",
       "      <td>4.428571</td>\n",
       "      <td>started reading one bathtub get id gotten fina...</td>\n",
       "      <td>started reading one bathtub get id gotten fina...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.993411</td>\n",
       "      <td>0.683657</td>\n",
       "      <td>-0.247203</td>\n",
       "      <td>0.033083</td>\n",
       "      <td>0.060892</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040029</td>\n",
       "      <td>-0.122108</td>\n",
       "      <td>0.089621</td>\n",
       "      <td>0.021345</td>\n",
       "      <td>0.117762</td>\n",
       "      <td>-0.058642</td>\n",
       "      <td>-0.083170</td>\n",
       "      <td>-0.118794</td>\n",
       "      <td>0.001701</td>\n",
       "      <td>-0.112346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>006016848X</td>\n",
       "      <td>3.562500</td>\n",
       "      <td>really like book time everyone want equality s...</td>\n",
       "      <td>really like book time everyone want equality s...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.977043</td>\n",
       "      <td>0.492497</td>\n",
       "      <td>0.233488</td>\n",
       "      <td>0.105423</td>\n",
       "      <td>0.168909</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038277</td>\n",
       "      <td>-0.107366</td>\n",
       "      <td>-0.157789</td>\n",
       "      <td>-0.082718</td>\n",
       "      <td>0.001719</td>\n",
       "      <td>-0.044761</td>\n",
       "      <td>0.076308</td>\n",
       "      <td>0.034848</td>\n",
       "      <td>-0.099364</td>\n",
       "      <td>-0.002617</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id  average_rating  \\\n",
       "0  0020425651        5.000000   \n",
       "1  0028610105        4.400000   \n",
       "2  006001203X        4.100000   \n",
       "3  0060096195        4.428571   \n",
       "4  006016848X        3.562500   \n",
       "\n",
       "                                  aggregated_reviews  \\\n",
       "0  susan cooper dark rising sequence joined pryda...   \n",
       "1  sheer diversity recipe japanese thai indian fr...   \n",
       "2  health care proffesional tell way traumatising...   \n",
       "3  started reading one bathtub get id gotten fina...   \n",
       "4  really like book time everyone want equality s...   \n",
       "\n",
       "                                  summarized_reviews  sentiment  \\\n",
       "0  susan cooper dark rising sequence joined pryda...          1   \n",
       "1  sheer diversity recipe japanese thai indian fr...          1   \n",
       "2  health care proffesional tell way traumatising...         -1   \n",
       "3  started reading one bathtub get id gotten fina...          1   \n",
       "4  really like book time everyone want equality s...         -1   \n",
       "\n",
       "   sentiment_score         0         1         2         3  ...        10  \\\n",
       "0         0.998871  0.635252 -0.269041  0.012194 -0.010467  ... -0.188243   \n",
       "1         0.975348  0.261072  0.044044  0.130149  0.080442  ... -0.076873   \n",
       "2         0.997175  0.257239  0.295288  0.238396  0.178317  ... -0.015927   \n",
       "3         0.993411  0.683657 -0.247203  0.033083  0.060892  ...  0.040029   \n",
       "4         0.977043  0.492497  0.233488  0.105423  0.168909  ... -0.038277   \n",
       "\n",
       "         11        12        13        14        15        16        17  \\\n",
       "0 -0.027312 -0.050258  0.066114 -0.039863 -0.032078  0.067193 -0.036900   \n",
       "1 -0.055025  0.061867  0.125944  0.289912  0.249448  0.005777  0.024197   \n",
       "2  0.056299  0.268024  0.118565  0.062791  0.051532  0.155097  0.039039   \n",
       "3 -0.122108  0.089621  0.021345  0.117762 -0.058642 -0.083170 -0.118794   \n",
       "4 -0.107366 -0.157789 -0.082718  0.001719 -0.044761  0.076308  0.034848   \n",
       "\n",
       "         18        19  \n",
       "0  0.138247  0.047895  \n",
       "1  0.114358  0.179739  \n",
       "2 -0.163372  0.166068  \n",
       "3  0.001701 -0.112346  \n",
       "4 -0.099364 -0.002617  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Set the desired number of components\n",
    "n_components = 20\n",
    "\n",
    "# Initialize TruncatedSVD and fit-transform the embeddings\n",
    "svd = TruncatedSVD(n_components=n_components)\n",
    "reduced_embeddings = svd.fit_transform(embeddings_df)\n",
    "\n",
    "# Convert the reduced embeddings to a DataFrame\n",
    "reduced_embeddings_df = pd.DataFrame(reduced_embeddings)\n",
    "\n",
    "# Concatenate the original data with the reduced embeddings\n",
    "vectorized_data_reduced = pd.concat([sample_data, reduced_embeddings_df], axis=1)\n",
    "vectorized_data_reduced.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "48feb37e-bf6b-4844-bad0-e85839c92ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 0.6891104190425241\n",
      "Train RMSE: 0.5050478806823305\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Assuming your final dataset is stored in 'vectorized_data'\n",
    "# and the relevant features start from the 6th column onwards (0-indexed)\n",
    "features = vectorized_data_reduced.iloc[:, 6:]\n",
    "\n",
    "# Step 1: Standardize the feature set\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "# Step 2: Split the data into training and testing sets\n",
    "train_data, test_data = train_test_split(vectorized_data_reduced, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reset indices for easier access\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "test_data = test_data.reset_index(drop=True)\n",
    "\n",
    "# Select vector features from the training data\n",
    "train_vector_features = scaled_features[train_data.index]\n",
    "\n",
    "# Calculate cosine similarity matrix for training data\n",
    "cosine_sim = cosine_similarity(train_vector_features)\n",
    "\n",
    "# Initialize a list to store the predicted ratings\n",
    "predicted_ratings = []\n",
    "\n",
    "# Step 4: Loop over each item in the test set to predict its rating\n",
    "for idx, row in test_data.iterrows():\n",
    "    # Extract the index of the current test item\n",
    "    test_index = row.name  # Get the original index in the dataset\n",
    "    \n",
    "    # Compute similarity between the test item and all training items\n",
    "    similarity_scores = cosine_sim[test_index, :]\n",
    "\n",
    "    # Get the indices of the top 5 most similar training items\n",
    "    similar_indices = np.argsort(similarity_scores)[::-1][:5]  # Top 5\n",
    "    \n",
    "    # Retrieve the ratings of these similar training items\n",
    "    similar_ratings = train_data.iloc[similar_indices]['average_rating']\n",
    "    \n",
    "    # Retrieve the corresponding similarity scores\n",
    "    similar_sim_scores = similarity_scores[similar_indices]\n",
    "    \n",
    "    # Compute the weighted average to predict the rating\n",
    "    if np.sum(similar_sim_scores) == 0:\n",
    "        # If similarity scores sum to zero, default to the mean rating of the training set\n",
    "        predicted_rating = train_data['average_rating'].mean()\n",
    "    else:\n",
    "        predicted_rating = np.dot(similar_sim_scores, similar_ratings) / np.sum(similar_sim_scores)\n",
    "    \n",
    "    # Append the predicted rating to the list\n",
    "    predicted_ratings.append(predicted_rating)\n",
    "\n",
    "# Add the predicted ratings to the test DataFrame\n",
    "test_data['predicted_rating'] = predicted_ratings\n",
    "\n",
    "# Step 5: Calculate RMSE on the test set\n",
    "rmse = np.sqrt(mean_squared_error(test_data['average_rating'], test_data['predicted_rating']))\n",
    "print(f\"Test RMSE: {rmse}\")\n",
    "\n",
    "# calculate RMSE for the training set \n",
    "train_predictions = []\n",
    "for idx, row in train_data.iterrows():\n",
    "    # Extract the index of the current train item\n",
    "    train_index = row.name  # Get the original index in the dataset\n",
    "    \n",
    "    # Compute similarity between the train item and all training items\n",
    "    similarity_scores = cosine_sim[train_index, :]\n",
    "\n",
    "    # Get the indices of the top 5 most similar training items\n",
    "    similar_indices = np.argsort(similarity_scores)[::-1][:5]  # Top 5\n",
    "    \n",
    "    # Retrieve the ratings of these similar training items\n",
    "    similar_ratings = train_data.iloc[similar_indices]['average_rating']\n",
    "    \n",
    "    # Retrieve the corresponding similarity scores\n",
    "    similar_sim_scores = similarity_scores[similar_indices]\n",
    "    \n",
    "    # Compute the weighted average to predict the rating\n",
    "    if np.sum(similar_sim_scores) == 0:\n",
    "        predicted_rating = train_data['average_rating'].mean()\n",
    "    else:\n",
    "        predicted_rating = np.dot(similar_sim_scores, similar_ratings) / np.sum(similar_sim_scores)\n",
    "    \n",
    "    # Append the predicted rating to the list\n",
    "    train_predictions.append(predicted_rating)\n",
    "\n",
    "# Add the predicted ratings to the train DataFrame\n",
    "train_data['predicted_rating'] = train_predictions\n",
    "\n",
    "# Calculate RMSE on the training set\n",
    "train_rmse = np.sqrt(mean_squared_error(train_data['average_rating'], train_data['predicted_rating']))\n",
    "print(f\"Train RMSE: {train_rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "fdbcf0fe-a198-463f-ab76-be007dd68973",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_data.to_csv('data/vectorized_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "7ec4eb19-4c9e-49ec-b308-ec1bad252f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 0.6891\n"
     ]
    }
   ],
   "source": [
    "# Running the model and calculate RMSE \n",
    "\n",
    "# the relevant features start from the 6th column onwards (0-indexed)\n",
    "features = vectorized_data_reduced.iloc[:, 6:]\n",
    "\n",
    "# Step 1: Standardize the feature set\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "# Step 2: Split the data into training and testing sets\n",
    "train_data, test_data = train_test_split(vectorized_data_reduced, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reset indices for easier access\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "test_data = test_data.reset_index(drop=True)\n",
    "\n",
    "# Select vector features from the training data\n",
    "train_vector_features = scaled_features[train_data.index]\n",
    "\n",
    "# Calculate cosine similarity matrix for training data\n",
    "cosine_sim = cosine_similarity(train_vector_features)\n",
    "\n",
    "# Initialize a list to store the predicted ratings\n",
    "predicted_ratings = []\n",
    "\n",
    "# Step 4: Loop over each item in the test set to predict its rating\n",
    "for idx, row in test_data.iterrows():\n",
    "    # Extract the index of the current test item\n",
    "    test_index = row.name  # Get the original index in the dataset\n",
    "    \n",
    "    # Compute similarity between the test item and all training items\n",
    "    similarity_scores = cosine_sim[test_index, :]\n",
    "\n",
    "    # Get the indices of the top 5 most similar training items\n",
    "    similar_indices = np.argsort(similarity_scores)[::-1][:5]  # Top 5\n",
    "    \n",
    "    # Retrieve the ratings of these similar training items\n",
    "    similar_ratings = train_data.iloc[similar_indices]['average_rating']\n",
    "    \n",
    "    # Retrieve the corresponding similarity scores\n",
    "    similar_sim_scores = similarity_scores[similar_indices]\n",
    "    \n",
    "    # Compute the weighted average to predict the rating\n",
    "    if np.sum(similar_sim_scores) == 0:\n",
    "        # If similarity scores sum to zero, default to the mean rating of the training set\n",
    "        predicted_rating = train_data['average_rating'].mean()\n",
    "    else:\n",
    "        predicted_rating = np.dot(similar_sim_scores, similar_ratings) / np.sum(similar_sim_scores)\n",
    "    \n",
    "    # Append the predicted rating to the list\n",
    "    predicted_ratings.append(predicted_rating)\n",
    "\n",
    "# Add the predicted ratings to the test DataFrame\n",
    "test_data['predicted_rating'] = predicted_ratings\n",
    "\n",
    "# Step 5: Calculate RMSE on the test set\n",
    "rmse = round(np.sqrt(mean_squared_error(test_data['average_rating'], test_data['predicted_rating'])),4)\n",
    "print(f\"Test RMSE: {rmse}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "38bf7c32-c2a1-4e60-8ee6-63dfb1382dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE: 0.505\n"
     ]
    }
   ],
   "source": [
    "# calculate RMSE for the training set \n",
    "train_predictions = []\n",
    "for idx, row in train_data.iterrows():\n",
    "    # Extract the index of the current train item\n",
    "    train_index = row.name  # Get the original index in the dataset\n",
    "    \n",
    "    # Compute similarity between the train item and all training items\n",
    "    similarity_scores = cosine_sim[train_index, :]\n",
    "\n",
    "    # Get the indices of the top 5 most similar training items\n",
    "    similar_indices = np.argsort(similarity_scores)[::-1][:5]  # Top 5\n",
    "    \n",
    "    # Retrieve the ratings of these similar training items\n",
    "    similar_ratings = train_data.iloc[similar_indices]['average_rating']\n",
    "    \n",
    "    # Retrieve the corresponding similarity scores\n",
    "    similar_sim_scores = similarity_scores[similar_indices]\n",
    "    \n",
    "    # Compute the weighted average to predict the rating\n",
    "    if np.sum(similar_sim_scores) == 0:\n",
    "        predicted_rating = train_data['average_rating'].mean()\n",
    "    else:\n",
    "        predicted_rating = np.dot(similar_sim_scores, similar_ratings) / np.sum(similar_sim_scores)\n",
    "    \n",
    "    # Append the predicted rating to the list\n",
    "    train_predictions.append(predicted_rating)\n",
    "\n",
    "# Add the predicted ratings to the train DataFrame\n",
    "train_data['predicted_rating'] = train_predictions\n",
    "\n",
    "# Calculate RMSE on the training set\n",
    "train_rmse = round(np.sqrt(mean_squared_error(train_data['average_rating'], train_data['predicted_rating'])),4)\n",
    "print(f\"Train RMSE: {train_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "a7cf33da-d61b-4478-addc-f85a9487cf63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      product_id  average_rating\n",
      "352   0345413350        4.526316\n",
      "763   0440995779        4.142857\n",
      "870   0451166582        4.428571\n",
      "104   0064406970        4.666667\n",
      "1093  0671024248        4.111111\n"
     ]
    }
   ],
   "source": [
    "def get_similar_products(product_id, top_n=5):\n",
    "    # Find the index of the given product ID\n",
    "    product_idx = vectorized_data_reduced[vectorized_data_reduced['product_id'] == product_id].index[0]\n",
    "    \n",
    "    # Get the similarity scores for this product\n",
    "    similarity_scores = cosine_sim[product_idx]\n",
    "    \n",
    "    # Get the indices of the most similar products, excluding the product itself\n",
    "    similar_indices = np.argsort(similarity_scores)[::-1][1:top_n + 1]\n",
    "    \n",
    "    # Get the similar products' details\n",
    "    similar_products = vectorized_data_reduced.iloc[similar_indices][['product_id', 'average_rating']]\n",
    "    \n",
    "    return similar_products\n",
    "\n",
    "# Example usage: Predict similar products for a given product ID\n",
    "similar_products = get_similar_products('0020425651', top_n=5)\n",
    "print(similar_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "ea8b40f5-5284-458f-b4ce-f5d2778ce7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>aggregated_reviews</th>\n",
       "      <th>summarized_reviews</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0020425651</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>susan cooper dark rising sequence joined pryda...</td>\n",
       "      <td>susan cooper dark rising sequence joined pryda...</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.998871</td>\n",
       "      <td>0.635252</td>\n",
       "      <td>-0.269045</td>\n",
       "      <td>0.012187</td>\n",
       "      <td>-0.010486</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007408</td>\n",
       "      <td>-0.130559</td>\n",
       "      <td>-0.088907</td>\n",
       "      <td>-0.097262</td>\n",
       "      <td>-0.004827</td>\n",
       "      <td>-0.097124</td>\n",
       "      <td>0.106259</td>\n",
       "      <td>0.008301</td>\n",
       "      <td>-0.028821</td>\n",
       "      <td>-0.006177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0028610105</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>sheer diversity recipe japanese thai indian fr...</td>\n",
       "      <td>sheer diversity recipe japanese thai indian fr...</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.975348</td>\n",
       "      <td>0.261072</td>\n",
       "      <td>0.044055</td>\n",
       "      <td>0.130192</td>\n",
       "      <td>0.080377</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.115873</td>\n",
       "      <td>0.020826</td>\n",
       "      <td>-0.093551</td>\n",
       "      <td>-0.112952</td>\n",
       "      <td>0.057351</td>\n",
       "      <td>0.000587</td>\n",
       "      <td>0.135271</td>\n",
       "      <td>0.006109</td>\n",
       "      <td>-0.027882</td>\n",
       "      <td>0.017403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>006001203X</td>\n",
       "      <td>4.100000</td>\n",
       "      <td>health care proffesional tell way traumatising...</td>\n",
       "      <td>health care proffesional tell way traumatising...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0.997175</td>\n",
       "      <td>0.257239</td>\n",
       "      <td>0.295279</td>\n",
       "      <td>0.238385</td>\n",
       "      <td>0.178327</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015217</td>\n",
       "      <td>-0.012752</td>\n",
       "      <td>0.036691</td>\n",
       "      <td>0.073162</td>\n",
       "      <td>-0.085413</td>\n",
       "      <td>0.005514</td>\n",
       "      <td>-0.008199</td>\n",
       "      <td>-0.041943</td>\n",
       "      <td>-0.069186</td>\n",
       "      <td>0.040457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0060096195</td>\n",
       "      <td>4.428571</td>\n",
       "      <td>started reading one bathtub get id gotten fina...</td>\n",
       "      <td>started reading one bathtub get id gotten fina...</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.993411</td>\n",
       "      <td>0.683657</td>\n",
       "      <td>-0.247196</td>\n",
       "      <td>0.033078</td>\n",
       "      <td>0.060899</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005285</td>\n",
       "      <td>0.064651</td>\n",
       "      <td>-0.008425</td>\n",
       "      <td>0.058347</td>\n",
       "      <td>-0.063713</td>\n",
       "      <td>-0.014532</td>\n",
       "      <td>-0.020493</td>\n",
       "      <td>-0.045625</td>\n",
       "      <td>0.054717</td>\n",
       "      <td>-0.064903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>006016848X</td>\n",
       "      <td>3.562500</td>\n",
       "      <td>really like book time everyone want equality s...</td>\n",
       "      <td>really like book time everyone want equality s...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0.977043</td>\n",
       "      <td>0.492497</td>\n",
       "      <td>0.233495</td>\n",
       "      <td>0.105419</td>\n",
       "      <td>0.168958</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.094077</td>\n",
       "      <td>0.050498</td>\n",
       "      <td>-0.065301</td>\n",
       "      <td>-0.012698</td>\n",
       "      <td>-0.071451</td>\n",
       "      <td>0.064249</td>\n",
       "      <td>-0.041519</td>\n",
       "      <td>-0.009113</td>\n",
       "      <td>0.018505</td>\n",
       "      <td>0.055179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id  average_rating  \\\n",
       "0  0020425651        5.000000   \n",
       "1  0028610105        4.400000   \n",
       "2  006001203X        4.100000   \n",
       "3  0060096195        4.428571   \n",
       "4  006016848X        3.562500   \n",
       "\n",
       "                                  aggregated_reviews  \\\n",
       "0  susan cooper dark rising sequence joined pryda...   \n",
       "1  sheer diversity recipe japanese thai indian fr...   \n",
       "2  health care proffesional tell way traumatising...   \n",
       "3  started reading one bathtub get id gotten fina...   \n",
       "4  really like book time everyone want equality s...   \n",
       "\n",
       "                                  summarized_reviews sentiment  \\\n",
       "0  susan cooper dark rising sequence joined pryda...  POSITIVE   \n",
       "1  sheer diversity recipe japanese thai indian fr...  POSITIVE   \n",
       "2  health care proffesional tell way traumatising...  NEGATIVE   \n",
       "3  started reading one bathtub get id gotten fina...  POSITIVE   \n",
       "4  really like book time everyone want equality s...  NEGATIVE   \n",
       "\n",
       "   sentiment_score         0         1         2         3  ...        40  \\\n",
       "0         0.998871  0.635252 -0.269045  0.012187 -0.010486  ...  0.007408   \n",
       "1         0.975348  0.261072  0.044055  0.130192  0.080377  ... -0.115873   \n",
       "2         0.997175  0.257239  0.295279  0.238385  0.178327  ...  0.015217   \n",
       "3         0.993411  0.683657 -0.247196  0.033078  0.060899  ...  0.005285   \n",
       "4         0.977043  0.492497  0.233495  0.105419  0.168958  ... -0.094077   \n",
       "\n",
       "         41        42        43        44        45        46        47  \\\n",
       "0 -0.130559 -0.088907 -0.097262 -0.004827 -0.097124  0.106259  0.008301   \n",
       "1  0.020826 -0.093551 -0.112952  0.057351  0.000587  0.135271  0.006109   \n",
       "2 -0.012752  0.036691  0.073162 -0.085413  0.005514 -0.008199 -0.041943   \n",
       "3  0.064651 -0.008425  0.058347 -0.063713 -0.014532 -0.020493 -0.045625   \n",
       "4  0.050498 -0.065301 -0.012698 -0.071451  0.064249 -0.041519 -0.009113   \n",
       "\n",
       "         48        49  \n",
       "0 -0.028821 -0.006177  \n",
       "1 -0.027882  0.017403  \n",
       "2 -0.069186  0.040457  \n",
       "3  0.054717 -0.064903  \n",
       "4  0.018505  0.055179  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load a pre-trained sentiment analysis model\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Function to get sentiment\n",
    "def get_sentiment(review):\n",
    "    result = sentiment_pipeline(review)[0]\n",
    "    return result['label'], result['score']\n",
    "\n",
    "# Apply the function to summarize the reviews\n",
    "vectorized_data_reduced[['sentiment', 'sentiment_score']] = vectorized_data_reduced['summarized_reviews'].apply(get_sentiment).apply(pd.Series)\n",
    "vectorized_data_reduced.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "698eb460-32e1-46bc-a350-fc0630ee411c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>aggregated_reviews</th>\n",
       "      <th>summarized_reviews</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0020425651</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>susan cooper dark rising sequence joined pryda...</td>\n",
       "      <td>susan cooper dark rising sequence joined pryda...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998871</td>\n",
       "      <td>0.635252</td>\n",
       "      <td>-0.269045</td>\n",
       "      <td>0.012187</td>\n",
       "      <td>-0.010486</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007408</td>\n",
       "      <td>-0.130559</td>\n",
       "      <td>-0.088907</td>\n",
       "      <td>-0.097262</td>\n",
       "      <td>-0.004827</td>\n",
       "      <td>-0.097124</td>\n",
       "      <td>0.106259</td>\n",
       "      <td>0.008301</td>\n",
       "      <td>-0.028821</td>\n",
       "      <td>-0.006177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0028610105</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>sheer diversity recipe japanese thai indian fr...</td>\n",
       "      <td>sheer diversity recipe japanese thai indian fr...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.975348</td>\n",
       "      <td>0.261072</td>\n",
       "      <td>0.044055</td>\n",
       "      <td>0.130192</td>\n",
       "      <td>0.080377</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.115873</td>\n",
       "      <td>0.020826</td>\n",
       "      <td>-0.093551</td>\n",
       "      <td>-0.112952</td>\n",
       "      <td>0.057351</td>\n",
       "      <td>0.000587</td>\n",
       "      <td>0.135271</td>\n",
       "      <td>0.006109</td>\n",
       "      <td>-0.027882</td>\n",
       "      <td>0.017403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>006001203X</td>\n",
       "      <td>4.100000</td>\n",
       "      <td>health care proffesional tell way traumatising...</td>\n",
       "      <td>health care proffesional tell way traumatising...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.997175</td>\n",
       "      <td>0.257239</td>\n",
       "      <td>0.295279</td>\n",
       "      <td>0.238385</td>\n",
       "      <td>0.178327</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015217</td>\n",
       "      <td>-0.012752</td>\n",
       "      <td>0.036691</td>\n",
       "      <td>0.073162</td>\n",
       "      <td>-0.085413</td>\n",
       "      <td>0.005514</td>\n",
       "      <td>-0.008199</td>\n",
       "      <td>-0.041943</td>\n",
       "      <td>-0.069186</td>\n",
       "      <td>0.040457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0060096195</td>\n",
       "      <td>4.428571</td>\n",
       "      <td>started reading one bathtub get id gotten fina...</td>\n",
       "      <td>started reading one bathtub get id gotten fina...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.993411</td>\n",
       "      <td>0.683657</td>\n",
       "      <td>-0.247196</td>\n",
       "      <td>0.033078</td>\n",
       "      <td>0.060899</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005285</td>\n",
       "      <td>0.064651</td>\n",
       "      <td>-0.008425</td>\n",
       "      <td>0.058347</td>\n",
       "      <td>-0.063713</td>\n",
       "      <td>-0.014532</td>\n",
       "      <td>-0.020493</td>\n",
       "      <td>-0.045625</td>\n",
       "      <td>0.054717</td>\n",
       "      <td>-0.064903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>006016848X</td>\n",
       "      <td>3.562500</td>\n",
       "      <td>really like book time everyone want equality s...</td>\n",
       "      <td>really like book time everyone want equality s...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.977043</td>\n",
       "      <td>0.492497</td>\n",
       "      <td>0.233495</td>\n",
       "      <td>0.105419</td>\n",
       "      <td>0.168958</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.094077</td>\n",
       "      <td>0.050498</td>\n",
       "      <td>-0.065301</td>\n",
       "      <td>-0.012698</td>\n",
       "      <td>-0.071451</td>\n",
       "      <td>0.064249</td>\n",
       "      <td>-0.041519</td>\n",
       "      <td>-0.009113</td>\n",
       "      <td>0.018505</td>\n",
       "      <td>0.055179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id  average_rating  \\\n",
       "0  0020425651        5.000000   \n",
       "1  0028610105        4.400000   \n",
       "2  006001203X        4.100000   \n",
       "3  0060096195        4.428571   \n",
       "4  006016848X        3.562500   \n",
       "\n",
       "                                  aggregated_reviews  \\\n",
       "0  susan cooper dark rising sequence joined pryda...   \n",
       "1  sheer diversity recipe japanese thai indian fr...   \n",
       "2  health care proffesional tell way traumatising...   \n",
       "3  started reading one bathtub get id gotten fina...   \n",
       "4  really like book time everyone want equality s...   \n",
       "\n",
       "                                  summarized_reviews  sentiment  \\\n",
       "0  susan cooper dark rising sequence joined pryda...          1   \n",
       "1  sheer diversity recipe japanese thai indian fr...          1   \n",
       "2  health care proffesional tell way traumatising...         -1   \n",
       "3  started reading one bathtub get id gotten fina...          1   \n",
       "4  really like book time everyone want equality s...         -1   \n",
       "\n",
       "   sentiment_score         0         1         2         3  ...        40  \\\n",
       "0         0.998871  0.635252 -0.269045  0.012187 -0.010486  ...  0.007408   \n",
       "1         0.975348  0.261072  0.044055  0.130192  0.080377  ... -0.115873   \n",
       "2         0.997175  0.257239  0.295279  0.238385  0.178327  ...  0.015217   \n",
       "3         0.993411  0.683657 -0.247196  0.033078  0.060899  ...  0.005285   \n",
       "4         0.977043  0.492497  0.233495  0.105419  0.168958  ... -0.094077   \n",
       "\n",
       "         41        42        43        44        45        46        47  \\\n",
       "0 -0.130559 -0.088907 -0.097262 -0.004827 -0.097124  0.106259  0.008301   \n",
       "1  0.020826 -0.093551 -0.112952  0.057351  0.000587  0.135271  0.006109   \n",
       "2 -0.012752  0.036691  0.073162 -0.085413  0.005514 -0.008199 -0.041943   \n",
       "3  0.064651 -0.008425  0.058347 -0.063713 -0.014532 -0.020493 -0.045625   \n",
       "4  0.050498 -0.065301 -0.012698 -0.071451  0.064249 -0.041519 -0.009113   \n",
       "\n",
       "         48        49  \n",
       "0 -0.028821 -0.006177  \n",
       "1 -0.027882  0.017403  \n",
       "2 -0.069186  0.040457  \n",
       "3  0.054717 -0.064903  \n",
       "4  0.018505  0.055179  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_mapping = {\n",
    "    'POSITIVE': 1,\n",
    "    'NEGATIVE': -1,\n",
    "    'NEUTRAL': 0  \n",
    "}\n",
    "vectorized_data_reduced['sentiment'] = vectorized_data_reduced['sentiment'].map(sentiment_mapping)\n",
    "vectorized_data_reduced.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:563]",
   "language": "python",
   "name": "conda-env-563-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
