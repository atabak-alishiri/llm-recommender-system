{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eec1218-5ab8-4639-8adf-b5a969693d9d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "## Introduction\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dbf6a5c-6590-4378-bdb9-b3c912dbf0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from hashlib import sha1\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from surprise import SVD\n",
    "from surprise import Dataset, Reader\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise import accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b961be-5f3c-4426-ab30-15b14a53cacc",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "## Data Description<a name=\"2\"></a>\n",
    "Given the large size of the dataset, only 10000 rows of the dataset is used for the models.\n",
    "This project utilizes a comprehensive dataset sourced from Kaggle, which can be accessed via the following link: (https://www.kaggle.com/datasets/beaglelee/amazon-reviews-us-books-v1-02-tsv-zip). The dataset consists of 15 columns and encompasses a substantial total of 3,105,370 rows, providing rich insights into customer feedback and product ratings specifically within the book category.\n",
    "\n",
    "Due to the extensive size of the dataset, a subset of 10,000 rows has been selected for analysis and modeling. This reduction allows for efficient processing while still capturing the diverse range of reviews and ratings present in the original dataset.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "7bbe7833-717b-4df6-8ea4-8f3b2f147ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "data = pd.read_csv(\"data/amazon_reviews_us_Books_v1_02.tsv\", sep='\\t', on_bad_lines='skip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "bf3228b3-7f08-4f8e-a746-7abd2473b066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>marketplace</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_category</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>vine</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>12076615</td>\n",
       "      <td>RQ58W7SMO911M</td>\n",
       "      <td>0385730586</td>\n",
       "      <td>122662979</td>\n",
       "      <td>Sisterhood of the Traveling Pants (Book 1)</td>\n",
       "      <td>Books</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>this book was a great learning novel!</td>\n",
       "      <td>this boook was a great one that you could lear...</td>\n",
       "      <td>2005-10-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>12703090</td>\n",
       "      <td>RF6IUKMGL8SF</td>\n",
       "      <td>0811828964</td>\n",
       "      <td>56191234</td>\n",
       "      <td>The Bad Girl's Guide to Getting What You Want</td>\n",
       "      <td>Books</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Fun Fluff</td>\n",
       "      <td>If you are looking for something to stimulate ...</td>\n",
       "      <td>2005-10-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>12257412</td>\n",
       "      <td>R1DOSHH6AI622S</td>\n",
       "      <td>1844161560</td>\n",
       "      <td>253182049</td>\n",
       "      <td>Eisenhorn (A Warhammer 40,000 Omnibus)</td>\n",
       "      <td>Books</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>this isn't a review</td>\n",
       "      <td>never read it-a young relative idicated he lik...</td>\n",
       "      <td>2005-10-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US</td>\n",
       "      <td>50732546</td>\n",
       "      <td>RATOTLA3OF70O</td>\n",
       "      <td>0373836635</td>\n",
       "      <td>348672532</td>\n",
       "      <td>Colby Conspiracy (Colby Agency)</td>\n",
       "      <td>Books</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>fine author on her A-game</td>\n",
       "      <td>Though she is honored to be Chicago Woman of t...</td>\n",
       "      <td>2005-10-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US</td>\n",
       "      <td>51964897</td>\n",
       "      <td>R1TNWRKIVHVYOV</td>\n",
       "      <td>0262181533</td>\n",
       "      <td>598678717</td>\n",
       "      <td>The Psychology of Proof: Deductive Reasoning i...</td>\n",
       "      <td>Books</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Execellent cursor examination</td>\n",
       "      <td>Review based on a cursory examination by Unive...</td>\n",
       "      <td>2005-10-14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  marketplace  customer_id       review_id  product_id  product_parent  \\\n",
       "0          US     12076615   RQ58W7SMO911M  0385730586       122662979   \n",
       "1          US     12703090    RF6IUKMGL8SF  0811828964        56191234   \n",
       "2          US     12257412  R1DOSHH6AI622S  1844161560       253182049   \n",
       "3          US     50732546   RATOTLA3OF70O  0373836635       348672532   \n",
       "4          US     51964897  R1TNWRKIVHVYOV  0262181533       598678717   \n",
       "\n",
       "                                       product_title product_category  \\\n",
       "0         Sisterhood of the Traveling Pants (Book 1)            Books   \n",
       "1      The Bad Girl's Guide to Getting What You Want            Books   \n",
       "2             Eisenhorn (A Warhammer 40,000 Omnibus)            Books   \n",
       "3                    Colby Conspiracy (Colby Agency)            Books   \n",
       "4  The Psychology of Proof: Deductive Reasoning i...            Books   \n",
       "\n",
       "   star_rating  helpful_votes  total_votes vine verified_purchase  \\\n",
       "0          4.0            2.0          3.0    N                 N   \n",
       "1          3.0            5.0          5.0    N                 N   \n",
       "2          4.0            1.0         22.0    N                 N   \n",
       "3          5.0            2.0          2.0    N                 N   \n",
       "4          4.0            0.0          2.0    N                 N   \n",
       "\n",
       "                         review_headline  \\\n",
       "0  this book was a great learning novel!   \n",
       "1                              Fun Fluff   \n",
       "2                    this isn't a review   \n",
       "3              fine author on her A-game   \n",
       "4          Execellent cursor examination   \n",
       "\n",
       "                                         review_body review_date  \n",
       "0  this boook was a great one that you could lear...  2005-10-14  \n",
       "1  If you are looking for something to stimulate ...  2005-10-14  \n",
       "2  never read it-a young relative idicated he lik...  2005-10-14  \n",
       "3  Though she is honored to be Chicago Woman of t...  2005-10-14  \n",
       "4  Review based on a cursory examination by Unive...  2005-10-14  "
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5e2f95-f813-4cd3-879c-34b3d3f9cfc1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "## Exploratory Data Analysis(EDA) <a name=\"3\"></a>\n",
    "\n",
    "This section describes the exploratory data analysis (EDA) techniques employed to derive valuable insights from the dataset, which will inform the subsequent stages of model development.\n",
    "\n",
    "To create a targeted subset for analysis, we identified product IDs and customer IDs associated with at least 100 reviews. This filtering process resulted in a dataset containing 24,466 rows, representing customer reviews. Our final subset includes 1,672 distinct products and 1,230 distinct customers, ensuring a diverse representation of both products and customers. This comprehensive approach enables us to conduct a thorough examination of customer feedback, facilitating deeper insights into their preferences and behaviors.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "bf4c5e63-e457-4f53-ba53-3b7d23e0686e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3105370 entries, 0 to 3105369\n",
      "Data columns (total 15 columns):\n",
      " #   Column             Dtype  \n",
      "---  ------             -----  \n",
      " 0   marketplace        object \n",
      " 1   customer_id        int64  \n",
      " 2   review_id          object \n",
      " 3   product_id         object \n",
      " 4   product_parent     int64  \n",
      " 5   product_title      object \n",
      " 6   product_category   object \n",
      " 7   star_rating        float64\n",
      " 8   helpful_votes      float64\n",
      " 9   total_votes        float64\n",
      " 10  vine               object \n",
      " 11  verified_purchase  object \n",
      " 12  review_headline    object \n",
      " 13  review_body        object \n",
      " 14  review_date        object \n",
      "dtypes: float64(3), int64(2), object(10)\n",
      "memory usage: 355.4+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "bcb0bdf6-8c2d-409f-9640-e625aefc7f38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "marketplace            0\n",
       "customer_id            0\n",
       "review_id              0\n",
       "product_id             0\n",
       "product_parent         0\n",
       "product_title          0\n",
       "product_category       0\n",
       "star_rating            4\n",
       "helpful_votes          4\n",
       "total_votes            4\n",
       "vine                   4\n",
       "verified_purchase      4\n",
       "review_headline       57\n",
       "review_body            4\n",
       "review_date          133\n",
       "dtype: int64"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "ed0a5f88-14af-4fd4-baf2-b22a8cd3c122",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "bd566ae0-7c36-40be-b983-4456e5c79188",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.replace(['null', 'N/A', '', ' '], np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "f8c3e030-ec22-4e02-9572-303fde59d229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "marketplace          0\n",
       "customer_id          0\n",
       "review_id            0\n",
       "product_id           0\n",
       "product_parent       0\n",
       "product_title        0\n",
       "product_category     0\n",
       "star_rating          0\n",
       "helpful_votes        0\n",
       "total_votes          0\n",
       "vine                 0\n",
       "verified_purchase    0\n",
       "review_headline      0\n",
       "review_body          0\n",
       "review_date          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "56ec071b-1471-4a84-adbf-8353cfa1f418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "marketplace                1\n",
       "customer_id          1502265\n",
       "review_id            3105184\n",
       "product_id            779692\n",
       "product_parent        666003\n",
       "product_title         713665\n",
       "product_category           1\n",
       "star_rating                5\n",
       "helpful_votes            942\n",
       "total_votes             1024\n",
       "vine                       2\n",
       "verified_purchase          2\n",
       "review_headline      2456998\n",
       "review_body          3070458\n",
       "review_date             3575\n",
       "dtype: int64"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "ee6755d9-ce24-4a8d-9a66-c8f580c147d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24466, 15)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selected a subset with customers and products with at least 100 reviews\n",
    "# Step 1: Filter customers with at least 100 reviews\n",
    "customer_review_counts = data.groupby('customer_id').size().reset_index(name='review_count')\n",
    "customers_with_at_least_100_reviews = customer_review_counts[customer_review_counts['review_count'] >= 100]\n",
    "\n",
    "# Step 2: Filter products with at least 100 reviews\n",
    "product_review_counts = data.groupby('product_id').size().reset_index(name='review_count')\n",
    "products_with_at_least_100_reviews = product_review_counts[product_review_counts['review_count'] >= 100]\n",
    "\n",
    "# Step 3: Filter the original dataset to only include customers and products with at least 100 reviews\n",
    "filtered_data = data[\n",
    "    (data['customer_id'].isin(customers_with_at_least_100_reviews['customer_id'])) &\n",
    "    (data['product_id'].isin(products_with_at_least_100_reviews['product_id']))\n",
    "]\n",
    "filtered_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "134c1fee-9f44-4bac-a943-4e4573faea09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>marketplace</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_category</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>vine</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>US</td>\n",
       "      <td>50230169</td>\n",
       "      <td>R23MCAR8GSV3T0</td>\n",
       "      <td>0451526341</td>\n",
       "      <td>380925201</td>\n",
       "      <td>Animal farm: A Fairy Story</td>\n",
       "      <td>Books</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Simple Yet Profound</td>\n",
       "      <td>A generation ago, the sight of the cover of Ge...</td>\n",
       "      <td>2005-10-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>US</td>\n",
       "      <td>50776149</td>\n",
       "      <td>RUCZYTA3MP0MR</td>\n",
       "      <td>038551428X</td>\n",
       "      <td>970964974</td>\n",
       "      <td>The Traveler (Fourth Realm Trilogy, Book 1)</td>\n",
       "      <td>Books</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Great Marketing for a Pretty Good Book</td>\n",
       "      <td>The most interesting thing about this book is ...</td>\n",
       "      <td>2005-10-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>US</td>\n",
       "      <td>12598621</td>\n",
       "      <td>RCL2ARHKWH6RL</td>\n",
       "      <td>059035342X</td>\n",
       "      <td>667539744</td>\n",
       "      <td>Harry Potter and the Sorcerer's Stone</td>\n",
       "      <td>Books</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>I Think Part Of The Charm Is You Feel Like You...</td>\n",
       "      <td>Even though this is the shortest book in the H...</td>\n",
       "      <td>2005-10-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>US</td>\n",
       "      <td>49770667</td>\n",
       "      <td>R2P4B3STC980QP</td>\n",
       "      <td>1594480001</td>\n",
       "      <td>659516630</td>\n",
       "      <td>The Kite Runner</td>\n",
       "      <td>Books</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Praiseworthy first novel</td>\n",
       "      <td>Well I thoroughly enjoyed this book. Although ...</td>\n",
       "      <td>2005-10-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>US</td>\n",
       "      <td>49828549</td>\n",
       "      <td>RM0CSYVWKHW5W</td>\n",
       "      <td>0671027360</td>\n",
       "      <td>141370518</td>\n",
       "      <td>Angels &amp; Demons</td>\n",
       "      <td>Books</td>\n",
       "      <td>1.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Preposterous</td>\n",
       "      <td>Early in this novel, our hero finds out that a...</td>\n",
       "      <td>2005-10-14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    marketplace  customer_id       review_id  product_id  product_parent  \\\n",
       "295          US     50230169  R23MCAR8GSV3T0  0451526341       380925201   \n",
       "307          US     50776149   RUCZYTA3MP0MR  038551428X       970964974   \n",
       "314          US     12598621   RCL2ARHKWH6RL  059035342X       667539744   \n",
       "363          US     49770667  R2P4B3STC980QP  1594480001       659516630   \n",
       "406          US     49828549   RM0CSYVWKHW5W  0671027360       141370518   \n",
       "\n",
       "                                   product_title product_category  \\\n",
       "295                   Animal farm: A Fairy Story            Books   \n",
       "307  The Traveler (Fourth Realm Trilogy, Book 1)            Books   \n",
       "314        Harry Potter and the Sorcerer's Stone            Books   \n",
       "363                              The Kite Runner            Books   \n",
       "406                              Angels & Demons            Books   \n",
       "\n",
       "     star_rating  helpful_votes  total_votes vine verified_purchase  \\\n",
       "295          4.0            2.0          2.0    N                 N   \n",
       "307          5.0            2.0          5.0    N                 N   \n",
       "314          5.0            2.0          2.0    N                 N   \n",
       "363          5.0            4.0          4.0    N                 N   \n",
       "406          1.0           31.0         39.0    N                 N   \n",
       "\n",
       "                                       review_headline  \\\n",
       "295                                Simple Yet Profound   \n",
       "307             Great Marketing for a Pretty Good Book   \n",
       "314  I Think Part Of The Charm Is You Feel Like You...   \n",
       "363                           Praiseworthy first novel   \n",
       "406                                       Preposterous   \n",
       "\n",
       "                                           review_body review_date  \n",
       "295  A generation ago, the sight of the cover of Ge...  2005-10-14  \n",
       "307  The most interesting thing about this book is ...  2005-10-14  \n",
       "314  Even though this is the shortest book in the H...  2005-10-14  \n",
       "363  Well I thoroughly enjoyed this book. Although ...  2005-10-14  \n",
       "406  Early in this novel, our hero finds out that a...  2005-10-14  "
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "64c9e215-042b-45e3-be92-6d84b8400d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data.to_csv('data/amazon_reviews_subset_100.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "f3c23737-7b42-4942-8e8e-7b819d3d22a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "marketplace              1\n",
       "customer_id           1230\n",
       "review_id            24466\n",
       "product_id            1672\n",
       "product_parent        1485\n",
       "product_title         1562\n",
       "product_category         1\n",
       "star_rating              5\n",
       "helpful_votes          423\n",
       "total_votes            460\n",
       "vine                     1\n",
       "verified_purchase        2\n",
       "review_headline      23305\n",
       "review_body          24314\n",
       "review_date           2574\n",
       "dtype: int64"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961a5e36-f4fd-45b6-8c43-6740dc262019",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "## Collaborative Filtering\n",
    "**Collaborative Filtering** is a widely-used technique for addressing the challenge of missing entries in a utility matrix, leveraging user behavior and interactions to make recommendations. This approach operates on the principle that users who have agreed in the past will continue to agree in the future, allowing the model to infer preferences based on the preferences of similar users.\n",
    "\n",
    "This method can be likened to advanced dimensionality reduction techniques such as Latent Semantic Analysis (LSA) or Truncated Singular Value Decomposition (SVD). By capturing the underlying relationships between users and items, collaborative filtering helps to predict missing values, enhancing the accuracy and relevance of recommendations.\n",
    "\n",
    "In this project, we will implement collaborative filtering as our baseline model to improve user experience by personalizing content based on historical data, thus enabling more informed decision-making.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "491d78cb-beef-48db-88bc-ddfb95474efd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>star_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50230169</td>\n",
       "      <td>0451526341</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50776149</td>\n",
       "      <td>038551428X</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12598621</td>\n",
       "      <td>059035342X</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49770667</td>\n",
       "      <td>1594480001</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49828549</td>\n",
       "      <td>0671027360</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  product_id  star_rating\n",
       "0     50230169  0451526341          4.0\n",
       "1     50776149  038551428X          5.0\n",
       "2     12598621  059035342X          5.0\n",
       "3     49770667  1594480001          5.0\n",
       "4     49828549  0671027360          1.0"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the data\n",
    "coll_data = filtered_data[['customer_id', 'product_id', 'star_rating']].reset_index(drop=True)\n",
    "coll_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "88259219-0172-4596-bfea-9f985d53d74e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "marketplace              1\n",
       "customer_id           1230\n",
       "review_id            24466\n",
       "product_id            1672\n",
       "product_parent        1485\n",
       "product_title         1562\n",
       "product_category         1\n",
       "star_rating              5\n",
       "helpful_votes          423\n",
       "total_votes            460\n",
       "vine                     1\n",
       "verified_purchase        2\n",
       "review_headline      23305\n",
       "review_body          24314\n",
       "review_date           2574\n",
       "dtype: int64"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "ad78779c-6dbe-4255-a4f8-efe18c29515a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customer_id    1230\n",
       "product_id     1672\n",
       "star_rating       5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coll_data.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "742f1968-46bc-490b-b7be-6a68094e4ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of customers (N)  : 1230\n",
      "Number of products (M) : 1672\n"
     ]
    }
   ],
   "source": [
    "# Number of customers and products\n",
    "user_key = \"customer_id\"\n",
    "item_key = \"product_id\"\n",
    "N = len(np.unique(coll_data[user_key])) \n",
    "M = len(np.unique(coll_data[item_key]))\n",
    "print(f\"Number of customers (N)  : {N}\")\n",
    "print(f\"Number of products (M) : {M}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "070d7eb1-ed77-4bba-b44b-f8fd9a0086dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-nan ratings percentage: 1.19\n"
     ]
    }
   ],
   "source": [
    "non_nan_ratings_percentage = (len(coll_data) / (N * M) * 100) \n",
    "print(f\"Non-nan ratings percentage: {np.round(non_nan_ratings_percentage,3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3ca5bd28-648d-4812-923c-1b97f4b711fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of ratings per customer : 19.89\n",
      "Average number of ratings per product: 14.63\n"
     ]
    }
   ],
   "source": [
    "avg_nratings_per_user = coll_data.groupby(user_key).size().mean()\n",
    "avg_nratings_per_movie = coll_data.groupby(item_key).size().mean()\n",
    "print(f\"Average number of ratings per customer : {avg_nratings_per_user:.2f}\")\n",
    "print(f\"Average number of ratings per product: {avg_nratings_per_movie:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "01a72bc1-78c6-4ead-b28b-abb0a17873a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Splitting\n",
    "X = coll_data.copy()\n",
    "y = coll_data['customer_id']\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "b70bb14a-b22d-4283-9d34-67b59d96b3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_mapper = dict(zip(np.unique(coll_data[user_key]), list(range(N))))\n",
    "item_mapper = dict(zip(np.unique(coll_data[item_key]), list(range(M))))\n",
    "user_inverse_mapper = dict(zip(list(range(N)), np.unique(coll_data[user_key])))\n",
    "item_inverse_mapper = dict(zip(list(range(M)), np.unique(coll_data[item_key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "c030f351-d414-410f-a4e2-be5ae058f3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_Y_from_ratings(data, N, M):\n",
    "    Y = np.zeros((N, M))\n",
    "    Y.fill(np.nan)\n",
    "    for index, val in data.iterrows():\n",
    "        n = user_mapper[val[user_key]]\n",
    "        m = item_mapper[val[item_key]]\n",
    "        Y[n, m] = val[\"star_rating\"]\n",
    "\n",
    "    return Y\n",
    "\n",
    "train_mat = create_Y_from_ratings(X_train, N, M)\n",
    "valid_mat = create_Y_from_ratings(X_valid, N, M)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "0446201b-ba26-4bdb-8006-f9852b0d56e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of non-nan elements in train_mat: 19085\n",
      "Number of non-nan elements in valid_mat: 4855\n"
     ]
    }
   ],
   "source": [
    "# What's the number of non-nan elements in train_mat (nnn_train_mat)?\n",
    "nnn_train_mat = np.sum(~np.isnan(train_mat)) \n",
    "\n",
    "# What's the number of non-nan elements in valid_mat (nnn_valid_mat)?\n",
    "nnn_valid_mat = np.sum(~np.isnan(valid_mat)) \n",
    "print(f\"Number of non-nan elements in train_mat: {nnn_train_mat}\")\n",
    "print(f\"Number of non-nan elements in valid_mat: {nnn_valid_mat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "329bf88f-8133-49e8-823a-a03c2e5022c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "def error(Y1, Y2):\n",
    "    \"\"\"\n",
    "    Given two matrices of the same shape, \n",
    "    returns the root mean squared error (RMSE).\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.nanmean((Y1 - Y2) ** 2))\n",
    "\n",
    "\n",
    "def evaluate(pred_Y, train_mat, valid_mat, model_name=\"Global average\"):\n",
    "    \"\"\"\n",
    "    Given predicted utility matrix and train and validation utility matrices \n",
    "    print train and validation RMSEs.\n",
    "    \"\"\"\n",
    "    print(\"%s train RMSE: %0.2f\" % (model_name, error(pred_Y, train_mat)))\n",
    "    print(\"%s valid RMSE: %0.2f\" % (model_name, error(pred_Y, valid_mat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "0a2c14fa-0053-4ff2-b3a5-6fd4aa05b08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global average train RMSE: 1.06\n",
      "Global average valid RMSE: 1.09\n"
     ]
    }
   ],
   "source": [
    "# global average rating baseline\n",
    "avg = np.nanmean(train_mat)\n",
    "pred_g = np.zeros(train_mat.shape) + avg\n",
    "evaluate(pred_g, train_mat, valid_mat, model_name=\"Global average\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "bfc44586-a17a-4aef-864d-a370f729bf5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-user average train RMSE: 0.94\n",
      "Per-user average valid RMSE: 1.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j6/44dd1c8s33xg2kn35nxq3f5m0000gn/T/ipykernel_48949/2152599090.py:2: RuntimeWarning: Mean of empty slice\n",
      "  avg_n = np.nanmean(train_mat, axis=1)\n"
     ]
    }
   ],
   "source": [
    "# Per-user average baseline\n",
    "avg_n = np.nanmean(train_mat, axis=1)\n",
    "avg_n[\n",
    "    np.isnan(avg_n)\n",
    "] = avg  \n",
    "pred_n = np.tile(avg_n[:, None], (1, M))\n",
    "evaluate(pred_n, train_mat, valid_mat, model_name=\"Per-user average\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "681a3704-cc72-433d-a3d1-83c1844c1563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-product average train RMSE: 0.93\n",
      "Per-product average valid RMSE: 1.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j6/44dd1c8s33xg2kn35nxq3f5m0000gn/T/ipykernel_48949/1388612677.py:2: RuntimeWarning: Mean of empty slice\n",
      "  avg_m = np.nanmean(train_mat, axis=0)\n"
     ]
    }
   ],
   "source": [
    "# Per-product average baseline\n",
    "avg_m = np.nanmean(train_mat, axis=0)\n",
    "avg_m[np.isnan(avg_m)] = avg\n",
    "pred_m = np.tile(avg_m[None, :], (N, 1))\n",
    "evaluate(pred_m, train_mat, valid_mat, model_name=\"Per-product average\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "ff5925cb-82d4-40de-ac29-dfdc496ceafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-user and product average train RMSE: 0.88\n",
      "Per-user and product average valid RMSE: 0.99\n"
     ]
    }
   ],
   "source": [
    "# Average of per-user and per-product average baselines\n",
    "pred_n_m = (pred_n + pred_m) * 0.5\n",
    "evaluate(pred_n_m, train_mat, valid_mat, model_name=\"Per-user and product average\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "705bb771-a661-4c89-9c40-909caf601848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of neighbours:  10\n",
      "Global average train RMSE: 0.00\n",
      "Global average valid RMSE: 1.08\n",
      "\n",
      "Number of neighbours:  15\n",
      "Global average train RMSE: 0.00\n",
      "Global average valid RMSE: 1.08\n",
      "\n",
      "Number of neighbours:  18\n",
      "Global average train RMSE: 0.00\n",
      "Global average valid RMSE: 1.08\n",
      "\n",
      "Number of neighbours:  20\n",
      "Global average train RMSE: 0.00\n",
      "Global average valid RMSE: 1.08\n",
      "\n",
      "Number of neighbours:  40\n",
      "Global average train RMSE: 0.00\n",
      "Global average valid RMSE: 1.08\n"
     ]
    }
   ],
   "source": [
    "# K-nearest neighbours imputation\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "num_neighs = [10, 15, 18, 20, 40]\n",
    "for n_neighbors in num_neighs:\n",
    "    print(\"\\nNumber of neighbours: \", n_neighbors)\n",
    "    imputer = KNNImputer(n_neighbors=n_neighbors, keep_empty_features=True)\n",
    "    pred_knn = imputer.fit_transform(train_mat)\n",
    "    evaluate(pred_knn, train_mat, valid_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "3294ab37-5558-4fed-ab30-8f9d1aac7ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "TruncatedSVD (k = 10) train RMSE: 0.82\n",
      "TruncatedSVD (k = 10) valid RMSE: 0.98\n",
      "\n",
      "\n",
      "TruncatedSVD (k = 50) train RMSE: 0.68\n",
      "TruncatedSVD (k = 50) valid RMSE: 0.98\n",
      "\n",
      "\n",
      "TruncatedSVD (k = 100) train RMSE: 0.56\n",
      "TruncatedSVD (k = 100) valid RMSE: 0.97\n",
      "\n",
      "\n",
      "TruncatedSVD (k = 200) train RMSE: 0.40\n",
      "TruncatedSVD (k = 200) valid RMSE: 0.97\n",
      "\n",
      "\n",
      "TruncatedSVD (k = 500) train RMSE: 0.15\n",
      "TruncatedSVD (k = 500) valid RMSE: 0.97\n",
      "\n",
      "\n",
      "TruncatedSVD (k = 1000) train RMSE: 0.01\n",
      "TruncatedSVD (k = 1000) valid RMSE: 0.97\n"
     ]
    }
   ],
   "source": [
    "# collaborative filtering with TruncatedSVD()\n",
    "def reconstruct_svd(Z, W, avg_n, avg_m):\n",
    "    return Z @ W + 0.5 * avg_n[:, None] + 0.5 * avg_m[None]\n",
    "\n",
    "\n",
    "train_mat_svd = train_mat - 0.5 * avg_n[:, None] - 0.5 * avg_m[None]\n",
    "train_mat_svd = np.nan_to_num(train_mat_svd)\n",
    "\n",
    "k_range = [10, 50, 100, 200, 500, 1000]\n",
    "for k in k_range:\n",
    "    print(\"\\n\")\n",
    "    tsvd = TruncatedSVD(n_components=k)\n",
    "    Z = tsvd.fit_transform(train_mat_svd)\n",
    "    W = tsvd.components_\n",
    "    X_hat = reconstruct_svd(Z, W, avg_n, avg_m)\n",
    "    evaluate(X_hat, train_mat, valid_mat, model_name=\"TruncatedSVD (k = %d)\" % k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "e24b2cfc-a3ce-43ea-927c-750370cab260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using surprise package\n",
    "reader = Reader()\n",
    "data = Dataset.load_from_df(coll_data, reader)  \n",
    "\n",
    "k = 10\n",
    "algo = SVD(n_factors=k, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "869fb9dd-5ae1-4a3b-b638-f34bb774cf7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RMSE of algorithm SVD on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    0.9642  0.9487  0.9521  0.9302  0.9462  0.9483  0.0109  \n",
      "Fit time          0.03    0.02    0.02    0.02    0.02    0.02    0.00    \n",
      "Test time         0.01    0.01    0.01    0.01    0.01    0.01    0.00    \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_rmse</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>test_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.964154</td>\n",
       "      <td>0.025022</td>\n",
       "      <td>0.009257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.948651</td>\n",
       "      <td>0.023375</td>\n",
       "      <td>0.008167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.952118</td>\n",
       "      <td>0.022660</td>\n",
       "      <td>0.008023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.930238</td>\n",
       "      <td>0.022565</td>\n",
       "      <td>0.007989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.946221</td>\n",
       "      <td>0.022534</td>\n",
       "      <td>0.007865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_rmse  fit_time  test_time\n",
       "0   0.964154  0.025022   0.009257\n",
       "1   0.948651  0.023375   0.008167\n",
       "2   0.952118  0.022660   0.008023\n",
       "3   0.930238  0.022565   0.007989\n",
       "4   0.946221  0.022534   0.007865"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(cross_validate(algo, data, measures=[\"RMSE\"], cv=5, verbose=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fdfa40-c5a4-4ee7-8b63-9cb1c031c7e4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "## Incorporating Reviews\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "1c5663cd-1d87-40b4-bcce-33393f8206e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50230169</td>\n",
       "      <td>0451526341</td>\n",
       "      <td>4.0</td>\n",
       "      <td>A generation ago, the sight of the cover of Ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50776149</td>\n",
       "      <td>038551428X</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The most interesting thing about this book is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12598621</td>\n",
       "      <td>059035342X</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Even though this is the shortest book in the H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49770667</td>\n",
       "      <td>1594480001</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Well I thoroughly enjoyed this book. Although ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49828549</td>\n",
       "      <td>0671027360</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Early in this novel, our hero finds out that a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  product_id  star_rating  \\\n",
       "0     50230169  0451526341          4.0   \n",
       "1     50776149  038551428X          5.0   \n",
       "2     12598621  059035342X          5.0   \n",
       "3     49770667  1594480001          5.0   \n",
       "4     49828549  0671027360          1.0   \n",
       "\n",
       "                                         review_body  \n",
       "0  A generation ago, the sight of the cover of Ge...  \n",
       "1  The most interesting thing about this book is ...  \n",
       "2  Even though this is the shortest book in the H...  \n",
       "3  Well I thoroughly enjoyed this book. Although ...  \n",
       "4  Early in this novel, our hero finds out that a...  "
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data = filtered_data[['customer_id', 'product_id', 'star_rating', 'review_body']].reset_index(drop=True)\n",
    "review_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "91118f4e-6303-4938-b649-46fc7e62fcf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24466, 4)"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "ddadfffb-02c4-438f-bc62-2f0500356f0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customer_id     1230\n",
       "product_id      1672\n",
       "star_rating        5\n",
       "review_body    24314\n",
       "dtype: int64"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a9d0ce39-eb3e-403a-bc96-1063b6b9cb5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>aggregated_reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0020425651</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>susan cooper dark rising sequence joined pryda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0028610105</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>sheer diversity recipe japanese thai indian fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>006001203X</td>\n",
       "      <td>4.100000</td>\n",
       "      <td>health care proffesional tell way traumatising...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0060096195</td>\n",
       "      <td>4.428571</td>\n",
       "      <td>started reading one bathtub get id gotten fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>006016848X</td>\n",
       "      <td>3.562500</td>\n",
       "      <td>really like book time everyone want equality s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id  average_rating  \\\n",
       "0  0020425651        5.000000   \n",
       "1  0028610105        4.400000   \n",
       "2  006001203X        4.100000   \n",
       "3  0060096195        4.428571   \n",
       "4  006016848X        3.562500   \n",
       "\n",
       "                                  aggregated_reviews  \n",
       "0  susan cooper dark rising sequence joined pryda...  \n",
       "1  sheer diversity recipe japanese thai indian fr...  \n",
       "2  health care proffesional tell way traumatising...  \n",
       "3  started reading one bathtub get id gotten fina...  \n",
       "4  really like book time everyone want equality s...  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean the reviews\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# Initialize stopwords and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Remove stopwords and lemmatize\n",
    "    text = ' '.join(lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words)\n",
    "    return text\n",
    "# Clean the 'review_body' column\n",
    "review_data['cleaned_review_body'] = review_data['review_body'].apply(clean_text)\n",
    "\n",
    "# Step 1: Group by 'customer_id' and 'product_id' and aggregate\n",
    "aggregated_data = review_data.groupby(['product_id']).agg(\n",
    "    average_rating=('star_rating', 'mean'),         # Mean of the star ratings\n",
    "    aggregated_reviews=('cleaned_review_body', ' '.join)  # Concatenate all cleaned review bodies\n",
    ").reset_index()\n",
    "\n",
    "# Display the aggregated DataFrame\n",
    "aggregated_data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3f968123-0cdb-4d99-8c46-97d5cc6f3cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "product_id            1672\n",
       "average_rating         425\n",
       "aggregated_reviews    1672\n",
       "summarized_reviews    1669\n",
       "dtype: int64"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_data.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ab2ae5b9-9913-44d1-84f1-3b67840ffbd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1672, 3)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "546a89ea-eb08-4054-ac6d-30e4898c5b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "# Load the BART model and tokenizer\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "58c0d1cd-207f-453d-83a5-7d5378727c2a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['customer_id'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m aggregated_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummarized_reviews\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m summarize_reviews(aggregated_data)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Display the result\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[43maggregated_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcustomer_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mproduct_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maverage_rating\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msummarized_reviews\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/563/lib/python3.11/site-packages/pandas/core/frame.py:4096\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4094\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4095\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4096\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4098\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/563/lib/python3.11/site-packages/pandas/core/indexes/base.py:6199\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6196\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6197\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6199\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6201\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6203\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/563/lib/python3.11/site-packages/pandas/core/indexes/base.py:6251\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6250\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6251\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['customer_id'] not in index\""
     ]
    }
   ],
   "source": [
    "\n",
    "def summarize_reviews(df):\n",
    "    summaries = []\n",
    "    for review in df['aggregated_reviews']:\n",
    "        inputs = tokenizer(review, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "        summary_ids = model.generate(inputs[\"input_ids\"], max_length=50, num_beams=4, early_stopping=True)\n",
    "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        summaries.append(summary)\n",
    "    return summaries\n",
    "\n",
    "# Summarizing reviews \n",
    "aggregated_data['summarized_reviews'] = summarize_reviews(aggregated_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "78767ec1-849c-49ae-ab25-ed83e9a5bc7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>summarized_reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0020425651</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>susan cooper dark rising sequence joined pryda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0028610105</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>sheer diversity recipe japanese thai indian fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>006001203X</td>\n",
       "      <td>4.100000</td>\n",
       "      <td>health care proffesional tell way traumatising...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0060096195</td>\n",
       "      <td>4.428571</td>\n",
       "      <td>started reading one bathtub get id gotten fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>006016848X</td>\n",
       "      <td>3.562500</td>\n",
       "      <td>really like book time everyone want equality s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1667</th>\n",
       "      <td>1931412065</td>\n",
       "      <td>4.875000</td>\n",
       "      <td>confirmed low carber year year constant raveno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1668</th>\n",
       "      <td>1931498717</td>\n",
       "      <td>4.727273</td>\n",
       "      <td>selection book group sunday september since or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1669</th>\n",
       "      <td>1931561648</td>\n",
       "      <td>4.437500</td>\n",
       "      <td>said time traveler wife nonconventional love s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1670</th>\n",
       "      <td>1931866007</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>book consists transcript interview mike litman...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1671</th>\n",
       "      <td>B0000WZWSI</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>first normally dont give many five star rating...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1672 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      product_id  average_rating  \\\n",
       "0     0020425651        5.000000   \n",
       "1     0028610105        4.400000   \n",
       "2     006001203X        4.100000   \n",
       "3     0060096195        4.428571   \n",
       "4     006016848X        3.562500   \n",
       "...          ...             ...   \n",
       "1667  1931412065        4.875000   \n",
       "1668  1931498717        4.727273   \n",
       "1669  1931561648        4.437500   \n",
       "1670  1931866007        5.000000   \n",
       "1671  B0000WZWSI        5.000000   \n",
       "\n",
       "                                     summarized_reviews  \n",
       "0     susan cooper dark rising sequence joined pryda...  \n",
       "1     sheer diversity recipe japanese thai indian fr...  \n",
       "2     health care proffesional tell way traumatising...  \n",
       "3     started reading one bathtub get id gotten fina...  \n",
       "4     really like book time everyone want equality s...  \n",
       "...                                                 ...  \n",
       "1667  confirmed low carber year year constant raveno...  \n",
       "1668  selection book group sunday september since or...  \n",
       "1669  said time traveler wife nonconventional love s...  \n",
       "1670  book consists transcript interview mike litman...  \n",
       "1671  first normally dont give many five star rating...  \n",
       "\n",
       "[1672 rows x 3 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_data[['product_id', 'average_rating', 'summarized_reviews']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d0a15928-db3d-40ca-9e50-1a5a84b855e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>summarized_reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0020425651</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>susan cooper dark rising sequence joined pryda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0028610105</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>sheer diversity recipe japanese thai indian fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>006001203X</td>\n",
       "      <td>4.100000</td>\n",
       "      <td>health care proffesional tell way traumatising...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0060096195</td>\n",
       "      <td>4.428571</td>\n",
       "      <td>started reading one bathtub get id gotten fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>006016848X</td>\n",
       "      <td>3.562500</td>\n",
       "      <td>really like book time everyone want equality s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1667</th>\n",
       "      <td>1931412065</td>\n",
       "      <td>4.875000</td>\n",
       "      <td>confirmed low carber year year constant raveno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1668</th>\n",
       "      <td>1931498717</td>\n",
       "      <td>4.727273</td>\n",
       "      <td>selection book group sunday september since or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1669</th>\n",
       "      <td>1931561648</td>\n",
       "      <td>4.437500</td>\n",
       "      <td>said time traveler wife nonconventional love s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1670</th>\n",
       "      <td>1931866007</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>book consists transcript interview mike litman...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1671</th>\n",
       "      <td>B0000WZWSI</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>first normally dont give many five star rating...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1672 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      product_id  average_rating  \\\n",
       "0     0020425651        5.000000   \n",
       "1     0028610105        4.400000   \n",
       "2     006001203X        4.100000   \n",
       "3     0060096195        4.428571   \n",
       "4     006016848X        3.562500   \n",
       "...          ...             ...   \n",
       "1667  1931412065        4.875000   \n",
       "1668  1931498717        4.727273   \n",
       "1669  1931561648        4.437500   \n",
       "1670  1931866007        5.000000   \n",
       "1671  B0000WZWSI        5.000000   \n",
       "\n",
       "                                     summarized_reviews  \n",
       "0     susan cooper dark rising sequence joined pryda...  \n",
       "1     sheer diversity recipe japanese thai indian fr...  \n",
       "2     health care proffesional tell way traumatising...  \n",
       "3     started reading one bathtub get id gotten fina...  \n",
       "4     really like book time everyone want equality s...  \n",
       "...                                                 ...  \n",
       "1667  confirmed low carber year year constant raveno...  \n",
       "1668  selection book group sunday september since or...  \n",
       "1669  said time traveler wife nonconventional love s...  \n",
       "1670  book consists transcript interview mike litman...  \n",
       "1671  first normally dont give many five star rating...  \n",
       "\n",
       "[1672 rows x 3 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_data[['product_id', 'average_rating', 'summarized_reviews']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "f5fb1915-1814-4f0e-8d0d-5358abaf719c",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_data.to_csv('data/summarized_review.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "582e645f-cc72-466a-8b84-88058830bed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data= aggregated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "28ecfc96-c663-4aa0-9493-cca3cbbf6609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>aggregated_reviews</th>\n",
       "      <th>summarized_reviews</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>...</th>\n",
       "      <th>374</th>\n",
       "      <th>375</th>\n",
       "      <th>376</th>\n",
       "      <th>377</th>\n",
       "      <th>378</th>\n",
       "      <th>379</th>\n",
       "      <th>380</th>\n",
       "      <th>381</th>\n",
       "      <th>382</th>\n",
       "      <th>383</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0020425651</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>susan cooper dark rising sequence joined pryda...</td>\n",
       "      <td>susan cooper dark rising sequence joined pryda...</td>\n",
       "      <td>-0.070799</td>\n",
       "      <td>-0.061837</td>\n",
       "      <td>-0.003631</td>\n",
       "      <td>0.012133</td>\n",
       "      <td>-0.035551</td>\n",
       "      <td>0.093195</td>\n",
       "      <td>...</td>\n",
       "      <td>0.065155</td>\n",
       "      <td>0.053727</td>\n",
       "      <td>0.003597</td>\n",
       "      <td>0.088892</td>\n",
       "      <td>-0.042067</td>\n",
       "      <td>0.041044</td>\n",
       "      <td>0.070728</td>\n",
       "      <td>-0.043085</td>\n",
       "      <td>-0.064512</td>\n",
       "      <td>0.038242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0028610105</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>sheer diversity recipe japanese thai indian fr...</td>\n",
       "      <td>sheer diversity recipe japanese thai indian fr...</td>\n",
       "      <td>-0.073018</td>\n",
       "      <td>-0.023593</td>\n",
       "      <td>0.066772</td>\n",
       "      <td>0.036159</td>\n",
       "      <td>0.006666</td>\n",
       "      <td>-0.010815</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042942</td>\n",
       "      <td>0.037886</td>\n",
       "      <td>-0.001067</td>\n",
       "      <td>-0.009074</td>\n",
       "      <td>0.065551</td>\n",
       "      <td>-0.054624</td>\n",
       "      <td>0.067726</td>\n",
       "      <td>0.079832</td>\n",
       "      <td>-0.015437</td>\n",
       "      <td>-0.041357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>006001203X</td>\n",
       "      <td>4.100000</td>\n",
       "      <td>health care proffesional tell way traumatising...</td>\n",
       "      <td>health care proffesional tell way traumatising...</td>\n",
       "      <td>0.004559</td>\n",
       "      <td>0.033951</td>\n",
       "      <td>0.020969</td>\n",
       "      <td>0.073957</td>\n",
       "      <td>-0.022270</td>\n",
       "      <td>0.056416</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005109</td>\n",
       "      <td>0.098458</td>\n",
       "      <td>0.004446</td>\n",
       "      <td>0.010148</td>\n",
       "      <td>-0.050168</td>\n",
       "      <td>0.036669</td>\n",
       "      <td>0.133147</td>\n",
       "      <td>0.013290</td>\n",
       "      <td>0.063390</td>\n",
       "      <td>0.043042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0060096195</td>\n",
       "      <td>4.428571</td>\n",
       "      <td>started reading one bathtub get id gotten fina...</td>\n",
       "      <td>started reading one bathtub get id gotten fina...</td>\n",
       "      <td>-0.066638</td>\n",
       "      <td>-0.083743</td>\n",
       "      <td>0.053587</td>\n",
       "      <td>0.066727</td>\n",
       "      <td>-0.009095</td>\n",
       "      <td>0.022768</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046725</td>\n",
       "      <td>0.029233</td>\n",
       "      <td>0.014859</td>\n",
       "      <td>0.084216</td>\n",
       "      <td>-0.085885</td>\n",
       "      <td>0.048461</td>\n",
       "      <td>0.023749</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>-0.075700</td>\n",
       "      <td>-0.034523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>006016848X</td>\n",
       "      <td>3.562500</td>\n",
       "      <td>really like book time everyone want equality s...</td>\n",
       "      <td>really like book time everyone want equality s...</td>\n",
       "      <td>-0.061388</td>\n",
       "      <td>0.048214</td>\n",
       "      <td>0.015069</td>\n",
       "      <td>-0.003492</td>\n",
       "      <td>-0.092024</td>\n",
       "      <td>0.022740</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041296</td>\n",
       "      <td>0.018716</td>\n",
       "      <td>0.018985</td>\n",
       "      <td>0.015178</td>\n",
       "      <td>-0.019459</td>\n",
       "      <td>0.000856</td>\n",
       "      <td>0.134094</td>\n",
       "      <td>-0.078086</td>\n",
       "      <td>0.004868</td>\n",
       "      <td>-0.001025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 388 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id  average_rating  \\\n",
       "0  0020425651        5.000000   \n",
       "1  0028610105        4.400000   \n",
       "2  006001203X        4.100000   \n",
       "3  0060096195        4.428571   \n",
       "4  006016848X        3.562500   \n",
       "\n",
       "                                  aggregated_reviews  \\\n",
       "0  susan cooper dark rising sequence joined pryda...   \n",
       "1  sheer diversity recipe japanese thai indian fr...   \n",
       "2  health care proffesional tell way traumatising...   \n",
       "3  started reading one bathtub get id gotten fina...   \n",
       "4  really like book time everyone want equality s...   \n",
       "\n",
       "                                  summarized_reviews         0         1  \\\n",
       "0  susan cooper dark rising sequence joined pryda... -0.070799 -0.061837   \n",
       "1  sheer diversity recipe japanese thai indian fr... -0.073018 -0.023593   \n",
       "2  health care proffesional tell way traumatising...  0.004559  0.033951   \n",
       "3  started reading one bathtub get id gotten fina... -0.066638 -0.083743   \n",
       "4  really like book time everyone want equality s... -0.061388  0.048214   \n",
       "\n",
       "          2         3         4         5  ...       374       375       376  \\\n",
       "0 -0.003631  0.012133 -0.035551  0.093195  ...  0.065155  0.053727  0.003597   \n",
       "1  0.066772  0.036159  0.006666 -0.010815  ...  0.042942  0.037886 -0.001067   \n",
       "2  0.020969  0.073957 -0.022270  0.056416  ... -0.005109  0.098458  0.004446   \n",
       "3  0.053587  0.066727 -0.009095  0.022768  ...  0.046725  0.029233  0.014859   \n",
       "4  0.015069 -0.003492 -0.092024  0.022740  ...  0.041296  0.018716  0.018985   \n",
       "\n",
       "        377       378       379       380       381       382       383  \n",
       "0  0.088892 -0.042067  0.041044  0.070728 -0.043085 -0.064512  0.038242  \n",
       "1 -0.009074  0.065551 -0.054624  0.067726  0.079832 -0.015437 -0.041357  \n",
       "2  0.010148 -0.050168  0.036669  0.133147  0.013290  0.063390  0.043042  \n",
       "3  0.084216 -0.085885  0.048461  0.023749  0.003057 -0.075700 -0.034523  \n",
       "4  0.015178 -0.019459  0.000856  0.134094 -0.078086  0.004868 -0.001025  \n",
       "\n",
       "[5 rows x 388 columns]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load a pre-trained model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2') \n",
    "\n",
    "# Encode the summaries to get embeddings\n",
    "embeddings = model.encode(sample_data['summarized_reviews'].tolist())\n",
    "\n",
    "# Convert embeddings to a DataFrame\n",
    "embeddings_df = pd.DataFrame(embeddings)\n",
    "\n",
    "vectorized_data = pd.concat([sample_data, embeddings_df], axis=1)\n",
    "vectorized_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "fdbcf0fe-a198-463f-ab76-be007dd68973",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_data.to_csv('data/vectorized_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "b4734a05-de71-486c-9651-8d150e9399a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>374</th>\n",
       "      <th>375</th>\n",
       "      <th>376</th>\n",
       "      <th>377</th>\n",
       "      <th>378</th>\n",
       "      <th>379</th>\n",
       "      <th>380</th>\n",
       "      <th>381</th>\n",
       "      <th>382</th>\n",
       "      <th>383</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.070799</td>\n",
       "      <td>-0.061837</td>\n",
       "      <td>-0.003631</td>\n",
       "      <td>0.012133</td>\n",
       "      <td>-0.035551</td>\n",
       "      <td>0.093195</td>\n",
       "      <td>-0.016502</td>\n",
       "      <td>-0.043888</td>\n",
       "      <td>0.047747</td>\n",
       "      <td>0.004803</td>\n",
       "      <td>...</td>\n",
       "      <td>0.065155</td>\n",
       "      <td>0.053727</td>\n",
       "      <td>0.003597</td>\n",
       "      <td>0.088892</td>\n",
       "      <td>-0.042067</td>\n",
       "      <td>0.041044</td>\n",
       "      <td>0.070728</td>\n",
       "      <td>-0.043085</td>\n",
       "      <td>-0.064512</td>\n",
       "      <td>0.038242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.073018</td>\n",
       "      <td>-0.023593</td>\n",
       "      <td>0.066772</td>\n",
       "      <td>0.036159</td>\n",
       "      <td>0.006666</td>\n",
       "      <td>-0.010815</td>\n",
       "      <td>0.028194</td>\n",
       "      <td>-0.013484</td>\n",
       "      <td>0.044148</td>\n",
       "      <td>-0.062531</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042942</td>\n",
       "      <td>0.037886</td>\n",
       "      <td>-0.001067</td>\n",
       "      <td>-0.009074</td>\n",
       "      <td>0.065551</td>\n",
       "      <td>-0.054624</td>\n",
       "      <td>0.067726</td>\n",
       "      <td>0.079832</td>\n",
       "      <td>-0.015437</td>\n",
       "      <td>-0.041357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.004559</td>\n",
       "      <td>0.033951</td>\n",
       "      <td>0.020969</td>\n",
       "      <td>0.073957</td>\n",
       "      <td>-0.022270</td>\n",
       "      <td>0.056416</td>\n",
       "      <td>0.097014</td>\n",
       "      <td>0.059796</td>\n",
       "      <td>-0.071576</td>\n",
       "      <td>0.020534</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005109</td>\n",
       "      <td>0.098458</td>\n",
       "      <td>0.004446</td>\n",
       "      <td>0.010148</td>\n",
       "      <td>-0.050168</td>\n",
       "      <td>0.036669</td>\n",
       "      <td>0.133147</td>\n",
       "      <td>0.013290</td>\n",
       "      <td>0.063390</td>\n",
       "      <td>0.043042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.066638</td>\n",
       "      <td>-0.083743</td>\n",
       "      <td>0.053587</td>\n",
       "      <td>0.066727</td>\n",
       "      <td>-0.009095</td>\n",
       "      <td>0.022768</td>\n",
       "      <td>0.044251</td>\n",
       "      <td>-0.016787</td>\n",
       "      <td>-0.010836</td>\n",
       "      <td>0.024886</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046725</td>\n",
       "      <td>0.029233</td>\n",
       "      <td>0.014859</td>\n",
       "      <td>0.084216</td>\n",
       "      <td>-0.085885</td>\n",
       "      <td>0.048461</td>\n",
       "      <td>0.023749</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>-0.075700</td>\n",
       "      <td>-0.034523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.061388</td>\n",
       "      <td>0.048214</td>\n",
       "      <td>0.015069</td>\n",
       "      <td>-0.003492</td>\n",
       "      <td>-0.092024</td>\n",
       "      <td>0.022740</td>\n",
       "      <td>0.012945</td>\n",
       "      <td>-0.029013</td>\n",
       "      <td>-0.006583</td>\n",
       "      <td>0.110357</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041296</td>\n",
       "      <td>0.018716</td>\n",
       "      <td>0.018985</td>\n",
       "      <td>0.015178</td>\n",
       "      <td>-0.019459</td>\n",
       "      <td>0.000856</td>\n",
       "      <td>0.134094</td>\n",
       "      <td>-0.078086</td>\n",
       "      <td>0.004868</td>\n",
       "      <td>-0.001025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 384 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0 -0.070799 -0.061837 -0.003631  0.012133 -0.035551  0.093195 -0.016502   \n",
       "1 -0.073018 -0.023593  0.066772  0.036159  0.006666 -0.010815  0.028194   \n",
       "2  0.004559  0.033951  0.020969  0.073957 -0.022270  0.056416  0.097014   \n",
       "3 -0.066638 -0.083743  0.053587  0.066727 -0.009095  0.022768  0.044251   \n",
       "4 -0.061388  0.048214  0.015069 -0.003492 -0.092024  0.022740  0.012945   \n",
       "\n",
       "        7         8         9    ...       374       375       376       377  \\\n",
       "0 -0.043888  0.047747  0.004803  ...  0.065155  0.053727  0.003597  0.088892   \n",
       "1 -0.013484  0.044148 -0.062531  ...  0.042942  0.037886 -0.001067 -0.009074   \n",
       "2  0.059796 -0.071576  0.020534  ... -0.005109  0.098458  0.004446  0.010148   \n",
       "3 -0.016787 -0.010836  0.024886  ...  0.046725  0.029233  0.014859  0.084216   \n",
       "4 -0.029013 -0.006583  0.110357  ...  0.041296  0.018716  0.018985  0.015178   \n",
       "\n",
       "        378       379       380       381       382       383  \n",
       "0 -0.042067  0.041044  0.070728 -0.043085 -0.064512  0.038242  \n",
       "1  0.065551 -0.054624  0.067726  0.079832 -0.015437 -0.041357  \n",
       "2 -0.050168  0.036669  0.133147  0.013290  0.063390  0.043042  \n",
       "3 -0.085885  0.048461  0.023749  0.003057 -0.075700 -0.034523  \n",
       "4 -0.019459  0.000856  0.134094 -0.078086  0.004868 -0.001025  \n",
       "\n",
       "[5 rows x 384 columns]"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select only the vectorized columns, assuming they start from the 4th column (index 4)\n",
    "book_feature = vectorized_data.iloc[:, 4:]\n",
    "\n",
    "# Display the first few rows to check the result\n",
    "book_feature.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "416b5e3f-48b7-494e-ae74-9c7f22f6d91c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1672, 384)"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "910ed511-47dd-4c0c-8aee-068042d2a34e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1672, 384)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_feats = book_feature.to_numpy()\n",
    "item_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "21452c0f-920d-4447-a520-61736a05311b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def get_X_y_per_user(filtered_data, d=item_feats.shape[1]):\n",
    "    \"\"\"\n",
    "    Returns X and y for each user.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    ratings : pandas.DataFrame\n",
    "         ratings data as a dataframe\n",
    "\n",
    "    d : int\n",
    "        number of item features\n",
    "\n",
    "    Return:\n",
    "    ----------\n",
    "        dictionaries containing X and y for all users\n",
    "    \"\"\"\n",
    "    lr_y = defaultdict(list)\n",
    "    lr_X = defaultdict(list)\n",
    "\n",
    "    for index, val in filtered_data.iterrows():\n",
    "        n = user_mapper[val[user_key]]\n",
    "        m = item_mapper[val[item_key]]\n",
    "        lr_X[n].append(item_feats[m])\n",
    "        lr_y[n].append(val[\"star_rating\"])\n",
    "\n",
    "    for n in lr_X:\n",
    "        lr_X[n] = np.array(lr_X[n])\n",
    "        lr_y[n] = np.array(lr_y[n])\n",
    "\n",
    "    return lr_X, lr_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "f5f1d613-d6dd-435b-af53-2aba44d7e992",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = item_feats.shape[1]\n",
    "X_train_usr, y_train_usr = get_X_y_per_user(X_train, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "ef39b7a5-9815-4206-a8eb-a4aa55b54bcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1215"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_usr.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "372d47ce-af4b-4a54-b2c4-397ce5850e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users with no reviews: 1\n"
     ]
    }
   ],
   "source": [
    "# Count users with no reviews (empty arrays)\n",
    "no_reviews_count = sum(1 for ratings in y_train_usr.values() if len(ratings) == 0)\n",
    "\n",
    "print(f\"Number of users with no reviews: {no_reviews_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "8588f57f-f556-42bc-b3c6-3084dd6a625a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users with no reviews: [59]\n"
     ]
    }
   ],
   "source": [
    "# Find users with no reviews (empty arrays)\n",
    "users_with_no_reviews = [user_id for user_id, ratings in y_train_usr.items() if len(ratings) == 0]\n",
    "\n",
    "print(f\"Users with no reviews: {users_with_no_reviews}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "66c8fe43-567f-4fa6-afc5-11c7ca538b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_usr[59]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "c98cd676-79df-4417-a796-1d1f5c06aa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "del y_train_usr[59]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "84066e5c-cdbf-4978-9f58-fa5e1e970255",
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train_usr[59]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "e1f6b1e8-9182-4038-a1d5-7cd84499f1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX user ID: 1105\n",
      "The count of ratings for MAX: 136\n"
     ]
    }
   ],
   "source": [
    "max_count = 0\n",
    "for key, value in X_train_usr.items():\n",
    "    result = len(value)\n",
    "    if result > max_count:\n",
    "        max_count = result\n",
    "        position = key\n",
    "\n",
    "print(f\"MAX user ID: {position}\")\n",
    "print(f\"The count of ratings for MAX: {max_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "bfd2eff1-d2e6-4bb1-b26c-d7890cc74337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIN user ID: 956\n",
      "The count of ratings for MIN: 1\n"
     ]
    }
   ],
   "source": [
    "min_count = float('inf') \n",
    "min_position = None\n",
    "\n",
    "for key, value in X_train_usr.items():\n",
    "    result = len(value)\n",
    "    if result < min_count:\n",
    "        min_count = result\n",
    "        min_position = key\n",
    "\n",
    "print(f\"MIN user ID: {min_position}\")\n",
    "print(f\"The count of ratings for MIN: {min_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "c1bcc3c0-59db-47fd-84ac-0251af4b3405",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[296], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N):\n\u001b[1;32m      7\u001b[0m     models[n] \u001b[38;5;241m=\u001b[39m Ridge()\n\u001b[0;32m----> 8\u001b[0m     \u001b[43mmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_usr\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_usr\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     pred_lin_reg[n] \u001b[38;5;241m=\u001b[39m models[n]\u001b[38;5;241m.\u001b[39mpredict(item_feats)\n\u001b[1;32m     11\u001b[0m evaluate(pred_lin_reg, train_mat, valid_mat, model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-based recommenders\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/563/lib/python3.11/site-packages/sklearn/base.py:1351\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1344\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1347\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1348\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1349\u001b[0m     )\n\u001b[1;32m   1350\u001b[0m ):\n\u001b[0;32m-> 1351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/563/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:1153\u001b[0m, in \u001b[0;36mRidge.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit Ridge regression model.\u001b[39;00m\n\u001b[1;32m   1134\u001b[0m \n\u001b[1;32m   1135\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[38;5;124;03m    Fitted estimator.\u001b[39;00m\n\u001b[1;32m   1151\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1152\u001b[0m _accept_sparse \u001b[38;5;241m=\u001b[39m _get_valid_accept_sparse(sparse\u001b[38;5;241m.\u001b[39missparse(X), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msolver)\n\u001b[0;32m-> 1153\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1156\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_accept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_numeric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "File \u001b[0;32m~/miniconda3/envs/563/lib/python3.11/site-packages/sklearn/base.py:650\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    648\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 650\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/miniconda3/envs/563/lib/python3.11/site-packages/sklearn/utils/validation.py:1192\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1187\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m   1188\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1190\u001b[0m     )\n\u001b[0;32m-> 1192\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1205\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1208\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m   1210\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m~/miniconda3/envs/563/lib/python3.11/site-packages/sklearn/utils/validation.py:989\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    982\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    983\u001b[0m             msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    984\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124marray=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    985\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    986\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    987\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif it contains a single sample.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    988\u001b[0m             )\n\u001b[0;32m--> 989\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m    991\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype_numeric \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(array\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkind\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSV\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    992\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    993\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    994\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    995\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "models = dict()\n",
    "pred_lin_reg = np.zeros((N, M))\n",
    "\n",
    "for n in range(N):\n",
    "    models[n] = Ridge()\n",
    "    models[n].fit(X_train_usr[n], y_train_usr[n])\n",
    "    pred_lin_reg[n] = models[n].predict(item_feats)\n",
    "    \n",
    "evaluate(pred_lin_reg, train_mat, valid_mat, model_name=\"Content-based recommenders\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "ffc26532-0dc9-4100-82af-ee43ca1c2a38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[181], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N):\n\u001b[1;32m     24\u001b[0m     models[n] \u001b[38;5;241m=\u001b[39m Ridge()\n\u001b[0;32m---> 25\u001b[0m     \u001b[43mmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_usr\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_usr\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     pred_lin_reg[n] \u001b[38;5;241m=\u001b[39m models[n]\u001b[38;5;241m.\u001b[39mpredict(Z)\n\u001b[1;32m     29\u001b[0m evaluate(pred_lin_reg, train_mat, valid_mat, model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-based recommender\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/563/lib/python3.11/site-packages/sklearn/base.py:1351\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1344\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1347\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1348\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1349\u001b[0m     )\n\u001b[1;32m   1350\u001b[0m ):\n\u001b[0;32m-> 1351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/563/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:1153\u001b[0m, in \u001b[0;36mRidge.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit Ridge regression model.\u001b[39;00m\n\u001b[1;32m   1134\u001b[0m \n\u001b[1;32m   1135\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[38;5;124;03m    Fitted estimator.\u001b[39;00m\n\u001b[1;32m   1151\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1152\u001b[0m _accept_sparse \u001b[38;5;241m=\u001b[39m _get_valid_accept_sparse(sparse\u001b[38;5;241m.\u001b[39missparse(X), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msolver)\n\u001b[0;32m-> 1153\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1156\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_accept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_numeric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "File \u001b[0;32m~/miniconda3/envs/563/lib/python3.11/site-packages/sklearn/base.py:650\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    648\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 650\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/miniconda3/envs/563/lib/python3.11/site-packages/sklearn/utils/validation.py:1192\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1187\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m   1188\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1190\u001b[0m     )\n\u001b[0;32m-> 1192\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1205\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1208\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m   1210\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m~/miniconda3/envs/563/lib/python3.11/site-packages/sklearn/utils/validation.py:989\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    982\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    983\u001b[0m             msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    984\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124marray=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    985\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    986\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    987\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif it contains a single sample.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    988\u001b[0m             )\n\u001b[0;32m--> 989\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m    991\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype_numeric \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(array\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkind\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSV\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    992\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    993\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    994\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    995\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "\n",
    "def train_for_usr(user_name, model=Ridge()):\n",
    "    X = X_train_usr[user_mapper[user_name]]\n",
    "    y = y_train_usr[user_mapper[user_name]]\n",
    "    model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict_for_usr(model, movie_names):\n",
    "    feat_vecs = movie_feats_df.loc[movie_names].values\n",
    "    preds = model.predict(feat_vecs)\n",
    "    return preds\n",
    "\n",
    "\n",
    "Z = book_feature.to_numpy()\n",
    "\n",
    "\n",
    "models = dict()\n",
    "pred_lin_reg = np.zeros((N, M))\n",
    "\n",
    "for n in range(N):\n",
    "    models[n] = Ridge()\n",
    "    models[n].fit(X_train_usr[n], y_train_usr[n])\n",
    "    pred_lin_reg[n] = models[n].predict(Z)\n",
    "\n",
    "\n",
    "evaluate(pred_lin_reg, train_mat, valid_mat, model_name=\"Content-based recommender\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e932c0ec-a684-4484-baab-421a719d1c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.6144474018351623\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Step 1: Preprocess the data\n",
    "# Select only the vectorized features for similarity calculation\n",
    "vector_features = vectorized_data.iloc[:, 4:] \n",
    "\n",
    "# Step 2: Calculate cosine similarity\n",
    "cosine_sim = cosine_similarity(vector_features)\n",
    "\n",
    "# Step 3: Predict ratings based on content similarity\n",
    "# Here, we predict the rating for a product based on the average rating of the most similar products\n",
    "\n",
    "# Initialize a list to store the predicted ratings\n",
    "predicted_ratings = []\n",
    "\n",
    "# Loop over each product to predict its rating\n",
    "for idx, row in vectorized_data.iterrows():\n",
    "    # Get the similarity scores for the current product\n",
    "    similarity_scores = cosine_sim[idx]\n",
    "    \n",
    "    # Get the indices of the most similar products (excluding the product itself)\n",
    "    similar_indices = np.argsort(similarity_scores)[::-1][1:6]  # Top 5 most similar products\n",
    "    \n",
    "    # Compute the weighted average rating based on the similarity scores\n",
    "    similar_ratings = vectorized_data.iloc[similar_indices]['average_rating']\n",
    "    similar_sim_scores = similarity_scores[similar_indices]\n",
    "    \n",
    "    # Avoid division by zero if the sum of similarity scores is zero\n",
    "    if np.sum(similar_sim_scores) == 0:\n",
    "        predicted_rating = np.mean(vectorized_data['average_rating'])  # Default to the average rating if no similarity\n",
    "    else:\n",
    "        predicted_rating = np.dot(similar_sim_scores, similar_ratings) / np.sum(similar_sim_scores)\n",
    "    \n",
    "    # Append the predicted rating\n",
    "    predicted_ratings.append(predicted_rating)\n",
    "\n",
    "# Add the predicted ratings to the original DataFrame\n",
    "vectorized_data['predicted_rating'] = predicted_ratings\n",
    "\n",
    "# Step 4: Evaluate the model using RMSE\n",
    "rmse = np.sqrt(mean_squared_error(vectorized_data['average_rating'], vectorized_data['predicted_rating']))\n",
    "print(f\"RMSE: {rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "09e7252d-60c5-49fd-b8fa-f222a8fe719b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 0.8494916187507252\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "train_data, test_data = train_test_split(vectorized_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reset indices for easier access\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "test_data = test_data.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Select vector features from the training data\n",
    "train_vector_features = train_data.iloc[:, 4:]\n",
    "\n",
    "# Calculate cosine similarity matrix for training data\n",
    "cosine_sim = cosine_similarity(train_vector_features)\n",
    "\n",
    "\n",
    "# Initialize a list to store the predicted ratings\n",
    "predicted_ratings = []\n",
    "\n",
    "# Loop over each item in the test set to predict its rating\n",
    "for idx, row in test_data.iterrows():\n",
    "    # Extract the vector features for the current test item\n",
    "    test_vector = row.iloc[4:].values.reshape(1, -1)\n",
    "    \n",
    "    # Compute similarity between the test item and all training items\n",
    "    similarity_scores = cosine_similarity(test_vector, train_vector_features).flatten()\n",
    "    \n",
    "    # Get the indices of the top 5 most similar training items\n",
    "    similar_indices = np.argsort(similarity_scores)[::-1][:5]  # Top 5\n",
    "    \n",
    "    # Retrieve the ratings of these similar training items\n",
    "    similar_ratings = train_data.iloc[similar_indices]['average_rating']\n",
    "    \n",
    "    # Retrieve the corresponding similarity scores\n",
    "    similar_sim_scores = similarity_scores[similar_indices]\n",
    "    \n",
    "    # Compute the weighted average to predict the rating\n",
    "    if np.sum(similar_sim_scores) == 0:\n",
    "        # If similarity scores sum to zero, default to the mean rating of the training set\n",
    "        predicted_rating = train_data['average_rating'].mean()\n",
    "    else:\n",
    "        predicted_rating = np.dot(similar_sim_scores, similar_ratings) / np.sum(similar_sim_scores)\n",
    "    \n",
    "    # Append the predicted rating to the list\n",
    "    predicted_ratings.append(predicted_rating)\n",
    "\n",
    "# Add the predicted ratings to the test DataFrame\n",
    "test_data['predicted_rating'] = predicted_ratings\n",
    "\n",
    "\n",
    "# Calculate RMSE on the test set\n",
    "rmse = np.sqrt(mean_squared_error(test_data['average_rating'], test_data['predicted_rating']))\n",
    "print(f\"Test RMSE: {rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "39f0ccb2-bcc4-4010-8ed4-829a3cea361b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE: 0.6202831718855597\n"
     ]
    }
   ],
   "source": [
    "# Initialize a list to store the predicted ratings for the training set\n",
    "train_predicted_ratings = []\n",
    "\n",
    "# Loop over each item in the training set to predict its rating\n",
    "for idx, row in train_data.iterrows():\n",
    "    # Extract the vector features for the current training item\n",
    "    current_vector = row.iloc[4:].values.reshape(1, -1)\n",
    "    \n",
    "    # Compute similarity between the current item and all other training items\n",
    "    similarity_scores = cosine_similarity(current_vector, train_vector_features).flatten()\n",
    "    \n",
    "    # Set the similarity score of the current item to zero to exclude it from the prediction\n",
    "    similarity_scores[idx] = 0\n",
    "    \n",
    "    # Get the indices of the top 5 most similar training items\n",
    "    similar_indices = np.argsort(similarity_scores)[::-1][:5]  # Top 5\n",
    "    \n",
    "    # Retrieve the ratings of these similar training items\n",
    "    similar_ratings = train_data.iloc[similar_indices]['average_rating']\n",
    "    \n",
    "    # Retrieve the corresponding similarity scores\n",
    "    similar_sim_scores = similarity_scores[similar_indices]\n",
    "    \n",
    "    # Compute the weighted average to predict the rating\n",
    "    if np.sum(similar_sim_scores) == 0:\n",
    "        # If similarity scores sum to zero, default to the mean rating of the training set\n",
    "        predicted_rating = train_data['average_rating'].mean()\n",
    "    else:\n",
    "        predicted_rating = np.dot(similar_sim_scores, similar_ratings) / np.sum(similar_sim_scores)\n",
    "    \n",
    "    # Append the predicted rating to the list\n",
    "    train_predicted_ratings.append(predicted_rating)\n",
    "\n",
    "# Add the predicted ratings to the training DataFrame\n",
    "train_data['predicted_rating'] = train_predicted_ratings\n",
    "\n",
    "# Calculate RMSE on the training set\n",
    "train_rmse = np.sqrt(mean_squared_error(train_data['average_rating'], train_data['predicted_rating']))\n",
    "print(f\"Train RMSE: {train_rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "a7cf33da-d61b-4478-addc-f85a9487cf63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      product_id  average_rating\n",
      "921   0471357634        4.666667\n",
      "1017  055357910X        3.727273\n",
      "977   055327449X        4.600000\n",
      "1302  0743476239        4.800000\n",
      "422   0375412824        3.933333\n"
     ]
    }
   ],
   "source": [
    "def get_similar_products(product_id, top_n=5):\n",
    "    # Find the index of the given product ID\n",
    "    product_idx = vectorized_data[vectorized_data['product_id'] == product_id].index[0]\n",
    "    \n",
    "    # Get the similarity scores for this product\n",
    "    similarity_scores = cosine_sim[product_idx]\n",
    "    \n",
    "    # Get the indices of the most similar products, excluding the product itself\n",
    "    similar_indices = np.argsort(similarity_scores)[::-1][1:top_n + 1]\n",
    "    \n",
    "    # Get the similar products' details\n",
    "    similar_products = vectorized_data.iloc[similar_indices][['product_id', 'average_rating']]\n",
    "    \n",
    "    return similar_products\n",
    "\n",
    "# Example usage: Predict similar products for a given product ID\n",
    "similar_products = get_similar_products('0020425651', top_n=5)\n",
    "print(similar_products)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8b40f5-5284-458f-b4ce-f5d2778ce7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load a pre-trained sentiment analysis model\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Function to get sentiment\n",
    "def get_sentiment(review):\n",
    "    result = sentiment_pipeline(review)[0]\n",
    "    return result['label'], result['score']\n",
    "\n",
    "# Apply the function to summarize the reviews\n",
    "sample_data[['sentiment', 'sentiment_score']] = sample_data['summarized_reviews'].apply(get_sentiment).apply(pd.Series)\n",
    "sample_data[['product_id', 'summarized_reviews','average_rating', 'sentiment', 'sentiment_score']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "698eb460-32e1-46bc-a350-fc0630ee411c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>summarized_reviews</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12598621</td>\n",
       "      <td>0060254920</td>\n",
       "      <td>5.0</td>\n",
       "      <td>hey dont waste time reading pedantic review fi...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.922823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12598621</td>\n",
       "      <td>0060740450</td>\n",
       "      <td>3.0</td>\n",
       "      <td>wanted love book id heard much somehow managed...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.963467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12598621</td>\n",
       "      <td>0064401847</td>\n",
       "      <td>4.0</td>\n",
       "      <td>thats say going sucky review earth katherine p...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.978294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12598621</td>\n",
       "      <td>0064407055</td>\n",
       "      <td>5.0</td>\n",
       "      <td>great great book sort novel curl snowing outsi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.979488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12598621</td>\n",
       "      <td>0140071083</td>\n",
       "      <td>5.0</td>\n",
       "      <td>easily best haunted house novel ever written h...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  product_id  average_rating  \\\n",
       "0     12598621  0060254920             5.0   \n",
       "1     12598621  0060740450             3.0   \n",
       "2     12598621  0064401847             4.0   \n",
       "3     12598621  0064407055             5.0   \n",
       "4     12598621  0140071083             5.0   \n",
       "\n",
       "                                  summarized_reviews  sentiment  \\\n",
       "0  hey dont waste time reading pedantic review fi...         -1   \n",
       "1  wanted love book id heard much somehow managed...         -1   \n",
       "2  thats say going sucky review earth katherine p...         -1   \n",
       "3  great great book sort novel curl snowing outsi...          1   \n",
       "4  easily best haunted house novel ever written h...          1   \n",
       "\n",
       "   sentiment_score  \n",
       "0         0.922823  \n",
       "1         0.963467  \n",
       "2         0.978294  \n",
       "3         0.979488  \n",
       "4         0.995321  "
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_mapping = {\n",
    "    'POSITIVE': 1,\n",
    "    'NEGATIVE': -1,\n",
    "    'NEUTRAL': 0  \n",
    "}\n",
    "sample_data['sentiment'] = sample_data['sentiment'].map(sentiment_mapping)\n",
    "sample_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "e5629642-1290-445d-a91c-552f7ecc2e6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>summarized_reviews</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>joy</th>\n",
       "      <th>fear</th>\n",
       "      <th>love</th>\n",
       "      <th>sadness</th>\n",
       "      <th>anger</th>\n",
       "      <th>surprise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12598621</td>\n",
       "      <td>0060254920</td>\n",
       "      <td>5.0</td>\n",
       "      <td>hey dont waste time reading pedantic review fi...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.922823</td>\n",
       "      <td>0.996757</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12598621</td>\n",
       "      <td>0060740450</td>\n",
       "      <td>3.0</td>\n",
       "      <td>wanted love book id heard much somehow managed...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.963467</td>\n",
       "      <td>0.998414</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12598621</td>\n",
       "      <td>0064401847</td>\n",
       "      <td>4.0</td>\n",
       "      <td>thats say going sucky review earth katherine p...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.978294</td>\n",
       "      <td>0.724140</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12598621</td>\n",
       "      <td>0064407055</td>\n",
       "      <td>5.0</td>\n",
       "      <td>great great book sort novel curl snowing outsi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.979488</td>\n",
       "      <td>0.987024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12598621</td>\n",
       "      <td>0140071083</td>\n",
       "      <td>5.0</td>\n",
       "      <td>easily best haunted house novel ever written h...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995321</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.95009</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  product_id  average_rating  \\\n",
       "0     12598621  0060254920             5.0   \n",
       "1     12598621  0060740450             3.0   \n",
       "2     12598621  0064401847             4.0   \n",
       "3     12598621  0064407055             5.0   \n",
       "4     12598621  0140071083             5.0   \n",
       "\n",
       "                                  summarized_reviews  sentiment  \\\n",
       "0  hey dont waste time reading pedantic review fi...         -1   \n",
       "1  wanted love book id heard much somehow managed...         -1   \n",
       "2  thats say going sucky review earth katherine p...         -1   \n",
       "3  great great book sort novel curl snowing outsi...          1   \n",
       "4  easily best haunted house novel ever written h...          1   \n",
       "\n",
       "   sentiment_score       joy     fear  love  sadness  anger  surprise  \n",
       "0         0.922823  0.996757      NaN   NaN      NaN    NaN       NaN  \n",
       "1         0.963467  0.998414      NaN   NaN      NaN    NaN       NaN  \n",
       "2         0.978294  0.724140      NaN   NaN      NaN    NaN       NaN  \n",
       "3         0.979488  0.987024      NaN   NaN      NaN    NaN       NaN  \n",
       "4         0.995321       NaN  0.95009   NaN      NaN    NaN       NaN  "
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a pre-trained emotion analysis model\n",
    "emotion_pipeline = pipeline(\"text-classification\", model=\"bhadresh-savani/bert-base-uncased-emotion\")\n",
    "\n",
    "# Function to get emotions\n",
    "def get_emotions(review):\n",
    "    result = emotion_pipeline(review)\n",
    "    return {emotion['label']: emotion['score'] for emotion in result}\n",
    "\n",
    "# Apply the function to get emotions\n",
    "emotion_df = sample_data['summarized_reviews'].apply(get_emotions).apply(pd.Series)\n",
    "\n",
    "# Join the emotion features back to the original DataFrame\n",
    "sample_data = pd.concat([sample_data, emotion_df], axis=1)\n",
    "sample_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "4983e127-ebc0-4ba9-bfcf-30d8820ae11b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>summarized_reviews</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>joy</th>\n",
       "      <th>fear</th>\n",
       "      <th>love</th>\n",
       "      <th>sadness</th>\n",
       "      <th>...</th>\n",
       "      <th>374</th>\n",
       "      <th>375</th>\n",
       "      <th>376</th>\n",
       "      <th>377</th>\n",
       "      <th>378</th>\n",
       "      <th>379</th>\n",
       "      <th>380</th>\n",
       "      <th>381</th>\n",
       "      <th>382</th>\n",
       "      <th>383</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12598621</td>\n",
       "      <td>0060254920</td>\n",
       "      <td>5.0</td>\n",
       "      <td>hey dont waste time reading pedantic review fi...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.922823</td>\n",
       "      <td>0.996757</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002508</td>\n",
       "      <td>0.062647</td>\n",
       "      <td>-0.012116</td>\n",
       "      <td>0.094314</td>\n",
       "      <td>-0.089067</td>\n",
       "      <td>0.022659</td>\n",
       "      <td>0.033685</td>\n",
       "      <td>-0.075982</td>\n",
       "      <td>-0.000192</td>\n",
       "      <td>0.016278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12598621</td>\n",
       "      <td>0060740450</td>\n",
       "      <td>3.0</td>\n",
       "      <td>wanted love book id heard much somehow managed...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.963467</td>\n",
       "      <td>0.998414</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006432</td>\n",
       "      <td>-0.018212</td>\n",
       "      <td>-0.074658</td>\n",
       "      <td>0.080583</td>\n",
       "      <td>-0.115148</td>\n",
       "      <td>-0.041499</td>\n",
       "      <td>0.113153</td>\n",
       "      <td>0.035668</td>\n",
       "      <td>-0.038790</td>\n",
       "      <td>0.057169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12598621</td>\n",
       "      <td>0064401847</td>\n",
       "      <td>4.0</td>\n",
       "      <td>thats say going sucky review earth katherine p...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.978294</td>\n",
       "      <td>0.724140</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082910</td>\n",
       "      <td>-0.012216</td>\n",
       "      <td>-0.009273</td>\n",
       "      <td>0.124878</td>\n",
       "      <td>-0.080804</td>\n",
       "      <td>0.083539</td>\n",
       "      <td>0.086977</td>\n",
       "      <td>0.021478</td>\n",
       "      <td>0.026029</td>\n",
       "      <td>-0.005424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12598621</td>\n",
       "      <td>0064407055</td>\n",
       "      <td>5.0</td>\n",
       "      <td>great great book sort novel curl snowing outsi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.979488</td>\n",
       "      <td>0.987024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049468</td>\n",
       "      <td>0.077545</td>\n",
       "      <td>0.033135</td>\n",
       "      <td>0.023243</td>\n",
       "      <td>-0.022011</td>\n",
       "      <td>0.007670</td>\n",
       "      <td>0.028730</td>\n",
       "      <td>-0.080761</td>\n",
       "      <td>-0.051240</td>\n",
       "      <td>0.011204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12598621</td>\n",
       "      <td>0140071083</td>\n",
       "      <td>5.0</td>\n",
       "      <td>easily best haunted house novel ever written h...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995321</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.95009</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071070</td>\n",
       "      <td>-0.000867</td>\n",
       "      <td>0.044181</td>\n",
       "      <td>0.009413</td>\n",
       "      <td>-0.051978</td>\n",
       "      <td>-0.047095</td>\n",
       "      <td>0.174457</td>\n",
       "      <td>0.033075</td>\n",
       "      <td>-0.090897</td>\n",
       "      <td>-0.009884</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 396 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  product_id  average_rating  \\\n",
       "0     12598621  0060254920             5.0   \n",
       "1     12598621  0060740450             3.0   \n",
       "2     12598621  0064401847             4.0   \n",
       "3     12598621  0064407055             5.0   \n",
       "4     12598621  0140071083             5.0   \n",
       "\n",
       "                                  summarized_reviews  sentiment  \\\n",
       "0  hey dont waste time reading pedantic review fi...         -1   \n",
       "1  wanted love book id heard much somehow managed...         -1   \n",
       "2  thats say going sucky review earth katherine p...         -1   \n",
       "3  great great book sort novel curl snowing outsi...          1   \n",
       "4  easily best haunted house novel ever written h...          1   \n",
       "\n",
       "   sentiment_score       joy     fear  love  sadness  ...       374       375  \\\n",
       "0         0.922823  0.996757      NaN   NaN      NaN  ... -0.002508  0.062647   \n",
       "1         0.963467  0.998414      NaN   NaN      NaN  ... -0.006432 -0.018212   \n",
       "2         0.978294  0.724140      NaN   NaN      NaN  ...  0.082910 -0.012216   \n",
       "3         0.979488  0.987024      NaN   NaN      NaN  ...  0.049468  0.077545   \n",
       "4         0.995321       NaN  0.95009   NaN      NaN  ...  0.071070 -0.000867   \n",
       "\n",
       "        376       377       378       379       380       381       382  \\\n",
       "0 -0.012116  0.094314 -0.089067  0.022659  0.033685 -0.075982 -0.000192   \n",
       "1 -0.074658  0.080583 -0.115148 -0.041499  0.113153  0.035668 -0.038790   \n",
       "2 -0.009273  0.124878 -0.080804  0.083539  0.086977  0.021478  0.026029   \n",
       "3  0.033135  0.023243 -0.022011  0.007670  0.028730 -0.080761 -0.051240   \n",
       "4  0.044181  0.009413 -0.051978 -0.047095  0.174457  0.033075 -0.090897   \n",
       "\n",
       "        383  \n",
       "0  0.016278  \n",
       "1  0.057169  \n",
       "2 -0.005424  \n",
       "3  0.011204  \n",
       "4 -0.009884  \n",
       "\n",
       "[5 rows x 396 columns]"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reset index if needed\n",
    "vectorized_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Merge the vectors with the main DataFrame on 'product_id'\n",
    "aggregated_data_with_vectors = pd.concat([sample_data, vectorized_data], axis=1)\n",
    "aggregated_data_with_vectors = aggregated_data_with_vectors.loc[:, ~aggregated_data_with_vectors.columns.duplicated()]\n",
    "\n",
    "aggregated_data_with_vectors.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "8bb43862-f6b0-4bfe-9f49-08562891f8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "review_vectors = aggregated_data_with_vectors.iloc[:, 9:].values \n",
    "\n",
    "# Function to predict ratings based on cosine similarity\n",
    "def predict_ratings(aggregated_data_with_vectors, review_vectors):\n",
    "    predicted_ratings = []\n",
    "\n",
    "    # Calculate cosine similarity matrix\n",
    "    similarity_matrix = cosine_similarity(review_vectors)\n",
    "\n",
    "    for idx in range(len(aggregated_data_with_vectors)):\n",
    "        # Get similar products (excluding itself)\n",
    "        similar_indices = np.argsort(similarity_matrix[idx])[:-2:-1]  # Get top similar products (excluding self)\n",
    "        \n",
    "        # Get ratings of similar products\n",
    "        similar_ratings = aggregated_data_with_vectors.iloc[similar_indices]['average_rating']\n",
    "        \n",
    "        # Predict rating as the mean of similar products' ratings\n",
    "        if len(similar_ratings) > 0:\n",
    "            predicted_rating = similar_ratings.mean()\n",
    "        else:\n",
    "            predicted_rating = aggregated_data_with_vectors.iloc[idx]['average_rating']  # Fallback to the original rating\n",
    "        \n",
    "        predicted_ratings.append(predicted_rating)\n",
    "\n",
    "    return predicted_ratings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "0d27b705-5396-4580-b746-0f323aed0660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Predict ratings\n",
    "predicted_ratings = predict_ratings(aggregated_data_with_vectors, review_vectors)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(aggregated_data_with_vectors['average_rating'], predicted_ratings))\n",
    "print(f'RMSE: {rmse:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ec0208-59dd-4f72-9442-14c3941471d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Select feature columns for content-based filtering\n",
    "feature_columns = list(range(10, 384))  # Assuming columns 10-383 are the vectorized features\n",
    "features = aggregated_data[feature_columns].fillna(0)  # Fill NaNs with zeros\n",
    "\n",
    "# Compute the cosine similarity matrix\n",
    "similarity_matrix = cosine_similarity(features)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:563]",
   "language": "python",
   "name": "conda-env-563-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
